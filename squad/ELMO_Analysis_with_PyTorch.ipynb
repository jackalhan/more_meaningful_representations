{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE PATHS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_as_fake = True\n",
    "\n",
    "#dataset_type = 'train'\n",
    "dataset_type = 'dev'\n",
    "dataset_version = 'v1.1'\n",
    "\n",
    "\n",
    "_basepath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/'\n",
    "_options_file_name = 'elmo_2x4096_512_2048cnn_2xhighway_weights.json'\n",
    "_weight_file_name = 'elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "_vocab_file_name = '{}_voc.txt'.format(dataset_type)\n",
    "_embedding_paragraph_file_as_h5py_name = 'elmo_paragraph_embeddings.hdf5'\n",
    "_embedding_question_file_as_h5py_name = 'elmo_question_embeddings.hdf5'\n",
    "_paragraphs_file_name_as_txt = '{}_paragraphs.txt'\n",
    "_questions_file_name_as_txt = '{}_questions.txt'\n",
    "_nearest_all_cos_similarity_results_file_name =  '{}_slice_{}_nearest_all_cos_similarity.csv'\n",
    "\n",
    "datadir = os.path.join(_basepath, dataset_type)\n",
    "modeldir = os.path.join(_basepath, 'model')\n",
    "_squad_file_name = '{}-{}.json'.format(dataset_type, dataset_version)\n",
    "squad_file = os.path.join(datadir, _squad_file_name)\n",
    "vocab_file = os.path.join(datadir, _vocab_file_name)\n",
    "options_file = os.path.join(modeldir, _options_file_name)\n",
    "weight_file = os.path.join(modeldir, _weight_file_name)\n",
    "embedding_paragraph_file_as_h5py = os.path.join(datadir, _embedding_paragraph_file_as_h5py_name)\n",
    "embedding_question_file_as_h5py = os.path.join(datadir, _embedding_question_file_as_h5py_name)\n",
    "paragraphs_file_as_txt = os.path.join(datadir, _paragraphs_file_name_as_txt.format(dataset_type))\n",
    "questions_file_as_txt = os.path.join(datadir, _questions_file_name_as_txt.format(dataset_type))\n",
    "nearest_all_cos_similarity_results_file = os.path.join(datadir, _nearest_all_cos_similarity_results_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITIES :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "index_field = ['Unnamed: 0']\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "    \n",
    "\n",
    "\n",
    "def read_squad_data(squad_file_path):\n",
    "    #Read Dataset From Json File\n",
    "    with open(squad_file, 'r') as _squad:\n",
    "        squad = json.load(_squad)\n",
    "    # Parse, titles and contents from the data    \n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    _i_para, _i_qas = 0, 0\n",
    "    for _i_titles, _titles in enumerate(squad['data']):\n",
    "        for _paragraph in _titles['paragraphs']:\n",
    "            paragraphs.append(_paragraph['context'].replace('\\n', ' '))\n",
    "            for _qas in _paragraph['qas']:\n",
    "                questions.append(_qas['question'].replace('\\n', ' '))                                  \n",
    "                _i_qas += 1\n",
    "            _i_para+=1\n",
    "    return paragraphs, questions\n",
    "\n",
    "def read_fake_data(paragraphs_file_path, \n",
    "                   questions_file_path):\n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    with open(paragraphs_file_path, 'r') as fp_in, open(questions_file_path, 'r') as fq_in:\n",
    "        for i, line in enumerate(fp_in):\n",
    "            paragraphs.append(line.replace('\\n', ' '))            \n",
    "        for i, line in enumerate(fq_in):\n",
    "            questions.append(line.replace('\\n', ' '))            \n",
    "    return paragraphs, questions\n",
    "    \n",
    "\n",
    "def tokenize_contexts(contexts:list):\n",
    "    tokenized_context = [word_tokenize(_) for _ in contexts]\n",
    "    return tokenized_context\n",
    "\n",
    "def dump_tokenized_contexts(tokenized_contexts:list, file_path:str):\n",
    "    with open(file_path, 'w') as fout:\n",
    "        for context in tokenized_contexts:\n",
    "            fout.write(' '.join(context) + '\\n')\n",
    "        fout.write('\\n')\n",
    "def create_voc(tokenized_contexts:list):\n",
    "    all_tokens = set(['<S>', '</S>', '<UNK>'])\n",
    "    for context in tokenized_contexts:\n",
    "        for token in context:\n",
    "            all_tokens.add(token)\n",
    "    return all_tokens\n",
    "\n",
    "def dump_voc(vocs:set, file_path:str):\n",
    "    with open(file_path, 'w') as fout:\n",
    "        fout.write('\\n'.join(vocs))\n",
    "        fout.write('\\n')\n",
    "        \n",
    "def create_and_dump_embeddings(embedder, \n",
    "                               tokenized_contexts_file_path:str, \n",
    "                               file_path_to_dump:str,\n",
    "                               embed_type='all'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    output_format : ``str``, optional, (default = \"all\")\n",
    "             The embeddings to output.  Must be one of \"all\", \"top\", or \"average\".\n",
    "    \"\"\"\n",
    "    with open(tokenized_contexts_file_path, 'r') as fin:         \n",
    "        ee.embed_file(fin, file_path_to_dump,output_format=embed_type)\n",
    "        \n",
    "def read_embeddings(embeddings_file_path, slice_index=None, axis=(0,1)):    \n",
    "    embeddings_=[]\n",
    "    with h5py.File(embeddings_file_path, 'r') as fin:        \n",
    "        print('Embeddings are getting processed!')\n",
    "        for _ in tqdm_notebook(fin, total=len(fin)):                        \n",
    "            vec = np.array(fin[_][...])            \n",
    "            if slice_index is not None:\n",
    "                vec = vec[slice_index]\n",
    "            mean_vector = np.apply_over_axes(np.mean, vec, axis)\n",
    "            embeddings_.append(mean_vector) \n",
    "    embeddings = np.asarray(embeddings_)\n",
    "    return embeddings \n",
    "\n",
    "def finding_nearest_neighbors(embedded_paragraphs_means, \n",
    "                              embedded_questions_means, \n",
    "                              questions, \n",
    "                              paragraphs,\n",
    "                              norm_type='l2'):\n",
    "    from sklearn.preprocessing import normalize\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print('Similarities are getting calculated !')   \n",
    "    nearest_neighbors = []\n",
    "    for q_id, _ in enumerate(tqdm_notebook(embedded_questions_means, total=len(embedded_questions_means))):\n",
    "        question = questions[q_id]\n",
    "        q_vec = np.array([_]) \n",
    "        if norm_type =='l2':\n",
    "            sk_sim = cosine_similarity(q_vec,embedded_paragraphs_means)[0]\n",
    "        else :\n",
    "            q_ = normalize(q_vec, norm='l1', axis=1)\n",
    "            p_ = normalize(embedded_paragraphs_means, norm='l1', axis=1)\n",
    "            sk_sim = np.dot(q_, p_.T)[0]\n",
    "        \n",
    "        similarities = np.argsort(-sk_sim)\n",
    "        order_of_the_actual_paragraph_id = np.where(similarities == q_id)[0][0] + 1\n",
    "        calculated_most_similar_1_paragraph = similarities[0]\n",
    "        for i, nearest_paragraph_id in enumerate(similarities[0:5]):\n",
    "            nearest_neighbors.append((question,\n",
    "                                       paragraphs[nearest_paragraph_id],\n",
    "                                       i+1, \n",
    "                                       sk_sim[nearest_paragraph_id] ))\n",
    "    return nearest_neighbors\n",
    "\n",
    "def dump_nearest_neighbors(nearest_neighbors:list, file_path:str):\n",
    "    df_nearest_neighbors = pd.DataFrame(data=nearest_neighbors, \n",
    "                                         columns=['question', \n",
    "                                                  'paragraph', \n",
    "                                                  'nearest_order', \n",
    "                                                  'cos_similarity'])\n",
    "    df_nearest_neighbors.to_csv(file_path, index=False)\n",
    "\n",
    "def traverse(o, tree_types=(list, tuple)):\n",
    "    if isinstance(o, tree_types):\n",
    "        for value in o:\n",
    "            for subvalue in traverse(value, tree_types):\n",
    "                yield subvalue\n",
    "    else:\n",
    "        yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs, questions = read_squad_data(squad_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND DUMP TOKENS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paragraphs = tokenize_contexts(paragraphs)\n",
    "dump_tokenized_contexts(tokenized_paragraphs, paragraphs_file_as_txt)\n",
    "\n",
    "tokenized_questions= tokenize_contexts(questions)\n",
    "dump_tokenized_contexts(tokenized_questions, questions_file_as_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND DUMP VOCABULARY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs = create_voc(tokenized_paragraphs + tokenized_questions)\n",
    "dump_voc(vocs, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKESET CREATION IF execution_type == 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute_as_fake:\n",
    "    token_counts = Counter([token for token in traverse(tokenized_paragraphs + tokenized_questions)])\n",
    "    token_counts = [(k, token_counts[k]) for k in sorted(token_counts, key=token_counts.get, reverse=True)]\n",
    "    print('Total length of tokens: {}'.format(len(token_counts)))\n",
    "    sanity_tokens = token_counts[100:5100]\n",
    "    print('Taking {} tokens from the list'.format(len(sanity_tokens)))\n",
    "    sanity_tokens = [[k] for k, v in sanity_tokens]\n",
    "    dump_tokenized_contexts(sanity_tokens, paragraphs_file_as_txt)\n",
    "    dump_tokenized_contexts(sanity_tokens, questions_file_as_txt)\n",
    "    paragraphs, questions = read_fake_data(paragraphs_file_as_txt, questions_file_as_txt)\n",
    "    dump_voc([k for k in traverse(sanity_tokens)], vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DUMP EMBEDDINGS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE ELMO EMBEDDER\n",
    "ee = ElmoEmbedder(options_file, weight_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: Before execute the following line to create embeddings as a batch, \n",
    "you need to make changes in the **embed_file** function of **elmo.py** file of the codes.\n",
    "The reason of doing it is that, instead of create datasets with token names in H5PY file, I am using indexes to store datasets in the file, therefore here is the small modification: \n",
    "\n",
    "**-> Line:285**\n",
    "\n",
    "**Original Code:**\n",
    "\n",
    "```python\n",
    "for key, embeddings in Tqdm.tqdm(embedded_sentences):\n",
    "    ...\n",
    "    ...\n",
    "    fout.create_dataset(key,\n",
    "                        output.shape, dtype='float32',\n",
    "                        data=output)\n",
    "```\n",
    "\n",
    "**Updated Code:**\n",
    "\n",
    "```python\n",
    "for i, embeddings_ in enumerate(Tqdm.tqdm(embedded_sentences)):\n",
    "    key = embeddings_[0]\n",
    "    embeddings = embeddings_[1]\n",
    "    ...\n",
    "    ...\n",
    "    fout.create_dataset(str(i),\n",
    "                        output.shape, dtype='float32',\n",
    "                        data=output)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_dump_embeddings(ee, paragraphs_file_as_txt, embedding_paragraph_file_as_h5py)\n",
    "create_and_dump_embeddings(ee, questions_file_as_txt, embedding_question_file_as_h5py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  EMBEDDINGS:\n",
    "#### SLICE CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 1024\n",
    "slices = [{'slice_type':'All',\n",
    "              'slice_index':None,\n",
    "              'axis':(0,1)},\n",
    "          {'slice_type':'1st',\n",
    "              'slice_index':0,\n",
    "              'axis':(0)},\n",
    "          {'slice_type':'2nd',\n",
    "              'slice_index':1,\n",
    "              'axis':(0)},\n",
    "          {'slice_type':'3rd',\n",
    "              'slice_index':2,\n",
    "              'axis':(0)}]\n",
    "\n",
    "selected_slice_conf =slices[0]\n",
    "\n",
    "print('Embeddings will be executed by the following configs: \\n{}'.format(selected_slice_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Paragraphs\n",
    "embedded_paragraphs = read_embeddings(embedding_paragraph_file_as_h5py, \n",
    "                                      selected_slice_conf['slice_index'], \n",
    "                                      selected_slice_conf['axis'])\n",
    "print('Paragraphs shape', embedded_paragraphs.shape)\n",
    "embedded_paragraphs_means_with_all_slices = np.reshape(embedded_paragraphs, \n",
    "                                                      (embedded_paragraphs.shape[0], dims))\n",
    "print('Paragraphs shape', embedded_paragraphs_means_with_all_slices.shape)\n",
    "# -------------------------- Questions\n",
    "embedded_questions = read_embeddings(embedding_question_file_as_h5py,\n",
    "                                     selected_slice_conf['slice_index'], \n",
    "                                     selected_slice_conf['axis'])\n",
    "print('Questions shape', embedded_questions.shape)\n",
    "embedded_questions_means_with_all_slices = np.reshape(embedded_questions, \n",
    "                                                      (embedded_questions.shape[0], dims))\n",
    "print('Questions shape', embedded_questions_means_with_all_slices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND and DUMP NEAREST NEIGHBORS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = finding_nearest_neighbors(embedded_paragraphs_means_with_all_slices, \n",
    "                          embedded_questions_means_with_all_slices, \n",
    "                          questions, \n",
    "                          paragraphs,\n",
    "                          norm_type='l2')\n",
    "\n",
    "dump_nearest_neighbors(nearest_neighbors, \n",
    "                       nearest_all_cos_similarity_results_file.format(dataset_type,selected_slice_conf['slice_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
