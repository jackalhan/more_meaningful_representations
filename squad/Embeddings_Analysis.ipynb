{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Best_Weights'\n",
    "each_file = 'elmo_only_weights_a_1_b_0_c0_output_neighbors.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECALL AT KS\n",
    "# mypath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Weights'\n",
    "# for (dirpath, dirnames, filenames) in os.walk(mypath):    \n",
    "#     performance = []\n",
    "#for each_file in filenames:\n",
    "performance = []\n",
    "print(each_file)\n",
    "neighbors = pd.read_csv(os.path.join(dirpath, each_file))\n",
    "neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "number_of_questions = len(neighbors.groupby('question'))\n",
    "neighbors.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "for k in [1,2,5,10,20,50]:\n",
    "    top_k = neighbors[neighbors['neighbor_order'] <= k]\n",
    "    len_of_received_relevant_doc = top_k[top_k['Is_Actual_Paragraph'] == 1]['Is_Actual_Paragraph'].count()\n",
    "    performance.append((k, each_file.replace('_output_neighbors.csv', ''), len_of_received_relevant_doc, len_of_received_relevant_doc/number_of_questions\n",
    "                        ))\n",
    "df_performance_model = pd.DataFrame(data=performance, columns=['top_n','conf', 'recall', 'normalized_recall'])\n",
    "df_performance_model.sort_values(by=['top_n', 'normalized_recall'], ascending=[True, False], inplace=True)\n",
    "df_performance_model.to_csv(os.path.join(dirpath, 'performances.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_score(best_score, original_score):\n",
    "    return original_score - 0.999999999 * best_score\n",
    "\n",
    "def calculate_precision_recall(row, number_of_questions):    \n",
    "    if row['Is_Actual_Paragraph'] == 1:\n",
    "        global global_relevant_counter\n",
    "        global_relevant_counter +=1\n",
    "    recall = global_relevant_counter/number_of_questions\n",
    "    precision = global_relevant_counter/(row.name + 1)\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_top1_top2_delta(top1_score, top2_score):\n",
    "    return top1_score - top2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P/R and MAP\n",
    "# for (dirpath, dirnames, filenames) in os.walk(mypath):    \n",
    "#     performance = []\n",
    "#     for each_file in filenames:\n",
    "print(each_file)\n",
    "cols = ['question','neighbor_paragraph','neighbor_order','neighbor_cos_similarity','actual_paragraph']\n",
    "neighbors = pd.read_csv(os.path.join(dirpath, each_file) , usecols=cols)\n",
    "neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "number_of_questions = int(len(neighbors.groupby('question')))\n",
    "number_of_paragraphs = int(len(neighbors) / number_of_questions)\n",
    "neighbors.sort_values(by=['question'], ascending=[True], inplace=True)\n",
    "neighbors['new_score'] = np.nan\n",
    "#x_main = pd.DataFrame()\n",
    "for _ in tqdm_notebook(range(0, int(number_of_questions))):\n",
    "    #x_sub = neighbors[neighbors['question'] == _]\n",
    "    _begin_index = _ * number_of_paragraphs\n",
    "    _end_index = _begin_index + number_of_paragraphs \n",
    "    #print(_begin_index, _end_index)\n",
    "    _max_score_of_this_q_set = neighbors.iloc[_begin_index:_end_index]['neighbor_cos_similarity'].max()\n",
    "    #_max_score_of_this_q_set = x['neighbor_cos_similarity'].max()    \n",
    "    neighbors.iloc[_begin_index:_end_index, neighbors.columns.get_loc('new_score')] = neighbors.iloc[_begin_index:_end_index].apply(lambda x : \n",
    "                                                                   calculate_new_score(_max_score_of_this_q_set, x['neighbor_cos_similarity']), axis=1)\n",
    "\n",
    "#     x_sub['new_score'] = x_sub.apply(lambda x : calculate_new_score(_max_score_of_this_q_set, \n",
    "#                                                                                        x['neighbor_cos_similarity']), axis=1)\n",
    "#     x_main = x_main.append(x_sub)\n",
    "\n",
    "neighbors.sort_values(by=['new_score'], ascending=[False], inplace=True)\n",
    "neighbors.to_csv(os.path.join(dirpath, 'new_scores_' + each_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECISION RECALL CALCULATION\n",
    "neighbors = pd.read_csv(os.path.join(dirpath, 'new_scores_' + each_file))    \n",
    "temp_df_new_scores = neighbors[neighbors['Is_Actual_Paragraph'] == 1]\n",
    "max_index = temp_df_new_scores['new_score'].idxmin()\n",
    "neighbors = neighbors[neighbors.index <= max_index]\n",
    "neighbors.sort_values(by=['new_score'], ascending=[False], inplace=True)\n",
    "neighbors = neighbors.reset_index(drop=True)\n",
    "number_of_questions = len(neighbors.groupby('question'))\n",
    "number_of_paragraphs = len(neighbors) / number_of_questions\n",
    "global_relevant_counter = 0\n",
    "neighbors['precision'], neighbors['recall'] = zip(*neighbors.apply(lambda x : calculate_precision_recall(x,number_of_questions), axis=1))\n",
    "df_precision_recall =neighbors[['precision', 'recall', 'Is_Actual_Paragraph']]\n",
    "df_precision_recall.to_csv(os.path.join(dirpath, 'pr_' + each_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AP CALCULATION AND CURVE\n",
    "df_precision_recall = pd.read_csv(os.path.join(dirpath, 'pr_' + each_file))  \n",
    "df_precision_recall_filtered = df_precision_recall[df_precision_recall['Is_Actual_Paragraph'] == 1]\n",
    "ap = df_precision_recall_filtered[\"precision\"].mean()\n",
    "ap\n",
    "plt.step(df_precision_recall_filtered['recall'], df_precision_recall_filtered['precision'], color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "plt.fill_between(df_precision_recall_filtered['recall'], df_precision_recall_filtered['precision'], step='post', alpha=0.2,\n",
    "                 color='b')   \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend([each_file.replace('_output_neighbors.csv', '')],loc=\"upper right\")\n",
    "plt.title('Precision-Recall curve for ' + str(number_of_questions) + ' docs from '+str(len(df_precision_recall))+' pairs : AP={0:0.2f}'.format(ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK DOUBLE CHECK BY USING THE SKLEARN LIBRARIES\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(neighbors['Is_Actual_Paragraph'], neighbors['new_score'])\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(neighbors['Is_Actual_Paragraph'], neighbors['new_score'])\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP1 - TOP2 DELTA\n",
    "#P/R and MAP\n",
    "# for (dirpath, dirnames, filenames) in os.walk(mypath):    \n",
    "#     performance = []\n",
    "#     for each_file in filenames:\n",
    "print(each_file)\n",
    "cols = ['question','neighbor_paragraph','neighbor_order','neighbor_cos_similarity','actual_paragraph']\n",
    "neighbors = pd.read_csv(os.path.join(dirpath, each_file) , usecols=cols)\n",
    "neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "number_of_questions = len(neighbors.groupby('question'))\n",
    "number_of_paragraphs = len(neighbors) / number_of_questions\n",
    "neighbors.sort_values(by=['question'], ascending=[True], inplace=True)\n",
    "neighbors['top1_top2_delta'] = np.nan\n",
    "for _ in tqdm_notebook(range(0, int(number_of_paragraphs))):  \n",
    "    _begin_index = _ * number_of_questions\n",
    "    _end_index = _begin_index + number_of_questions \n",
    "    _max_score_of_this_q_set = neighbors.iloc[_begin_index:_end_index]['neighbor_cos_similarity'].nlargest(1).values[0]\n",
    "    _second_max_score_of_this_q_set = neighbors.iloc[_begin_index:_end_index]['neighbor_cos_similarity'].nlargest(2).values[0]\n",
    "    neighbors.iloc[_begin_index:_end_index, neighbors.columns.get_loc('top1_top2_delta')] = neighbors.iloc[_begin_index:_end_index].apply(lambda x : \n",
    "                                                                   calculate_top1_top2_delta(_max_score_of_this_q_set, \n",
    "                                                                                       _second_max_score_of_this_q_set), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.float_info.epsilon\n",
    "x = neighbors[neighbors['Is_Actual_Paragraph'] == 1]\n",
    "len(x[x['top1_top2_delta'] > sys.float_info.epsilon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors.iloc[_begin_index:_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_second_max_score_of_this_q_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
