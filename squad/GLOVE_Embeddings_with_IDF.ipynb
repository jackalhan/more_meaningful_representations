{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import json\n",
    "import h5py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from shutil import copyfile\n",
    "nlp = spacy.blank(\"en\")\n",
    "encoding=\"utf-8\"\n",
    "tokenize = lambda doc: [token.text for token in nlp(doc)]\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "dataset_type = 'dev'\n",
    "dataset_version = 'v1.1'\n",
    "\n",
    "_basepath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/'\n",
    "datadir = os.path.join(_basepath, dataset_type)\n",
    "pre_trained_dir = os.path.join(_basepath, 'GLOVE', 'data')\n",
    "_paragraphs_file_name = '{}_glove_paragraphs.txt'.format(dataset_type)\n",
    "paragraphs_file = os.path.join(datadir, _paragraphs_file_name)\n",
    "\n",
    "_paragraph_embeddings_file_name = '{}_glove_paragraph_embeddings.hdf5'.format(dataset_type)\n",
    "paragraph_embeddings_file = os.path.join(datadir, _paragraph_embeddings_file_name)\n",
    "\n",
    "_token_embeddings_file_name = '{}_glove_token_embeddings.hdf5'.format(dataset_type)\n",
    "token_embeddings_file= os.path.join(datadir, _token_embeddings_file_name )\n",
    "\n",
    "_token_embeddings_guideline_file_name = '{}_glove_token_embeddings_guideline.pkl'.format(dataset_type)\n",
    "token_embeddings_guideline_file = os.path.join(datadir, _token_embeddings_guideline_file_name)\n",
    "\n",
    "_questions_file_name = '{}_glove_questions.txt'.format(dataset_type)\n",
    "questions_file = os.path.join(datadir, _questions_file_name)\n",
    "\n",
    "_question_embeddings_file_name = '{}_glove_question_embeddings.hdf5'.format(dataset_type)\n",
    "question_embeddings_file = os.path.join(datadir, _question_embeddings_file_name)\n",
    "\n",
    "_word_embeddings_file_name = '{}_glove_word_embeddings.hdf5'.format(dataset_type)\n",
    "word_embeddings_file = os.path.join(datadir, _word_embeddings_file_name)\n",
    "\n",
    "_neighbors_file_name = '{}_glove_neighbors.csv'.format(dataset_type)\n",
    "neighbors_file = os.path.join(datadir, _neighbors_file_name)\n",
    "\n",
    "_voc_file_name = '{}_voc.txt'.format(dataset_type)\n",
    "voc_file_name = os.path.join(datadir, _voc_file_name)\n",
    "\n",
    "_squad_file_name = '{}-{}.json'.format(dataset_type, dataset_version)\n",
    "squad_file = os.path.join(datadir, _squad_file_name)\n",
    "\n",
    "_squad_test_file_name = '{}-{}.json'.format('train', dataset_version)\n",
    "squad_test_file = os.path.join(datadir, _squad_test_file_name)\n",
    "\n",
    "_glove_file_name = 'GloVe.840B.300d.txt'\n",
    "glove_file = os.path.join(pre_trained_dir,'GloVe.840B.300d', _glove_file_name)\n",
    "\n",
    "def read_squad_data(squad_file_path):\n",
    "\n",
    "    #Read Dataset From Json File\n",
    "    with open(squad_file_path, 'r') as _squad:\n",
    "        squad = json.load(_squad)\n",
    "    # Parse, titles and contents from the data\n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    question_to_paragraph = []\n",
    "    _i_para, _i_qas = 0, 0\n",
    "    for _i_titles, _titles in enumerate(squad['data']):\n",
    "        for _paragraph in _titles['paragraphs']:\n",
    "            paragraphs.append(_paragraph['context'])\n",
    "            for _qas in _paragraph['qas']:\n",
    "                questions.append(_qas['question'])\n",
    "                question_to_paragraph.append(_i_para)\n",
    "                _i_qas += 1\n",
    "            _i_para+=1\n",
    "\n",
    "    return paragraphs, questions, question_to_paragraph\n",
    "\n",
    "def dump_tokenized_contexts(tokenized_contexts:list, file_path:str):\n",
    "    with open(file_path, 'w') as fout:\n",
    "        for context in tokenized_contexts:\n",
    "            fout.write(' '.join(context) + '\\n')\n",
    "\n",
    "def tokenize_contexts(contexts:list):\n",
    "    tokenized_context = [word_tokenize(context.strip()) for context in contexts]\n",
    "    return tokenized_context\n",
    "\n",
    "def calculate_similarity_and_dump(paragraphs_embeddings, questions_embeddings, slice_type, q_to_p, outfile):\n",
    "    neighbor_list = []\n",
    "    for _id, _q_embedding in enumerate(tqdm(questions_embeddings, total=len(questions_embeddings))):\n",
    "        _q_embedding = np.array([_q_embedding])\n",
    "        sk_sim = cosine_similarity(_q_embedding, paragraphs_embeddings)[0]\n",
    "        neighbors = np.argsort(-sk_sim)\n",
    "        for _, neighbor_id in enumerate(neighbors):\n",
    "            neighbor_list.append((slice_type,\n",
    "                                  _id,\n",
    "                                  neighbor_id,\n",
    "                                  _ + 1,\n",
    "                                  sk_sim[neighbor_id],\n",
    "                                  q_to_p[_id],\n",
    "                                  np.where(neighbors == q_to_p[_id])[0][0] + 1,\n",
    "                                  sk_sim[q_to_p[_id]]\n",
    "                                  ))\n",
    "    df_neighbors = pd.DataFrame(data=neighbor_list, columns=['slice_type',\n",
    "                                                             'question',\n",
    "                                                             'neighbor_paragraph',\n",
    "                                                             'neighbor_order',\n",
    "                                                             'neighbor_cos_similarity',\n",
    "                                                             'actual_paragraph',\n",
    "                                                             'actual_paragraph_order',\n",
    "                                                             'actual_paragrraph_cos_similarity'\n",
    "                                                             ])\n",
    "    df_neighbors.to_csv(outfile, index=False)\n",
    "    return df_neighbors\n",
    "def read_file(file_name):\n",
    "    with open(file_name) as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    return content\n",
    "\n",
    "def dump_embeddings(embeddings, outfile_to_dump):\n",
    "    with h5py.File(outfile_to_dump, 'w') as fout:\n",
    "        ds = fout.create_dataset(\n",
    "            'embeddings',\n",
    "            embeddings.shape, dtype='float32',\n",
    "            data=embeddings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [{'slice_type':'All', 'slice_index':None, 'axis':(1,2)},\n",
    "          {'slice_type':'1st', 'slice_index':0, 'axis':(1)},\n",
    "          {'slice_type':'2nd', 'slice_index':1, 'axis':(1)},\n",
    "          {'slice_type':'3rd', 'slice_index':2, 'axis':(1)}]\n",
    "\n",
    "s = slices[0] # option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=300\n",
    "tokens = []\n",
    "if voc_file_name is not None:\n",
    "    tokens = read_file(voc_file_name)\n",
    "glove_word_weights = {}\n",
    "with open(glove_file, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        token = parts[0].decode(encoding)\n",
    "        if len(tokens) >0:\n",
    "            if token in tokens:\n",
    "                nums = np.array(parts[1:], dtype=np.float32)\n",
    "                glove_word_weights[token] = nums\n",
    "        else:\n",
    "            nums = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_word_weights[token] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squad Data: Reading Dev Started\n",
      "# of Paragraphs : 2067\n",
      "# of Questions : 10570\n",
      "# of Q_to_P : 10570\n",
      "Squad Data: Reading Dev Ended in 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Squad Data: Reading Dev Started')\n",
    "start = datetime.datetime.now()\n",
    "paragraphs, questions, q_to_p = read_squad_data(squad_file)\n",
    "paragraphs_test, questions_test, q_to_p_test = read_squad_data(squad_test_file)\n",
    "end = datetime.datetime.now()\n",
    "print('# of Paragraphs : {}'.format(len(paragraphs)))\n",
    "print('# of Questions : {}'.format(len(questions)))\n",
    "print('# of Q_to_P : {}'.format(len(q_to_p)))\n",
    "print('Squad Data: Reading Dev Ended in {} minutes'.format((end-start).seconds/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started\n",
      "# of Tokenized Paragraphs: 2067\n",
      "Paragraphs: Tokenization and Saving Tokenization  is Completed in 0.5833333333333334 minutes\n"
     ]
    }
   ],
   "source": [
    "print(20* '-')\n",
    "print('Paragraphs: Tokenization and Saving Tokenization Started')\n",
    "start = datetime.datetime.now()\n",
    "tokenized_paragraphs = tokenize_contexts(paragraphs)\n",
    "tokenized_test_paragraphs = tokenize_contexts(paragraphs_test)\n",
    "dump_tokenized_contexts(tokenized_paragraphs, paragraphs_file)\n",
    "end = datetime.datetime.now()\n",
    "print('# of Tokenized Paragraphs: {}'.format(len(tokenized_paragraphs)))\n",
    "print('Paragraphs: Tokenization and Saving Tokenization  is Completed in {} minutes'.format((end-start).seconds/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started\n",
      "# of Tokenized Questions: 10570\n",
      "Questions: Tokenization and Saving Tokenization  is Completed in 0.25 minutes\n"
     ]
    }
   ],
   "source": [
    "print(20* '-')\n",
    "print('Questions: Tokenization and Saving Tokenization Started')\n",
    "start = datetime.datetime.now()\n",
    "tokenized_questions = tokenize_contexts(questions)\n",
    "tokenized_test_questions = tokenize_contexts(questions_test)\n",
    "dump_tokenized_contexts(tokenized_questions,questions_file)\n",
    "end = datetime.datetime.now()\n",
    "print('# of Tokenized Questions: {}'.format(len(tokenized_questions)))\n",
    "print('Questions: Tokenization and Saving Tokenization  is Completed in {} minutes'.format((end-start).seconds/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_nontokenized = [\" \".join(context) for context in tokenized_questions]\n",
    "paragraphs_nontokenized = [\" \".join(context) for context in tokenized_paragraphs]\n",
    "questions_test_nontokenized = [\" \".join(context) for context in tokenized_test_questions]\n",
    "paragraphs_test_nontokenized = [\" \".join(context) for context in tokenized_test_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tfidf_weights = None\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x, smooth_idf=True, sublinear_tf=True, tokenizer=tokenize)\n",
    "tfidf.fit(questions_nontokenized+paragraphs_nontokenized+questions_test_nontokenized+paragraphs_test_nontokenized)\n",
    "max_idf = max(tfidf.idf_)\n",
    "token2idfweight = defaultdict(\n",
    "    lambda: max_idf,\n",
    "    [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which',\n",
       " 'NFL',\n",
       " 'team',\n",
       " 'represented',\n",
       " 'the',\n",
       " 'AFC',\n",
       " 'at',\n",
       " 'Super',\n",
       " 'Bowl',\n",
       " '50',\n",
       " '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_glove_embeddings= np.array([\n",
    "                                np.mean([glove_word_weights[w] for w in words if w in glove_word_weights]\n",
    "                                or [np.zeros(dim)], axis=0)\n",
    "                                for words in tokenized_questions+tokenized_paragraphs\n",
    "                                ])\n",
    "####################################################################################################################\n",
    "##############       MEAN GLOVE EMBEDDINGS\n",
    "####################################################################################################################\n",
    "dump_embeddings(mean_glove_embeddings, os.path.join(datadir,'dev_mean_glove_embeddings.hdf5'))\n",
    "question_embeddings = mean_glove_embeddings[0:len(tokenized_questions),:]\n",
    "paragraphs_embeddings = mean_glove_embeddings[len(tokenized_questions):,:]\n",
    "print('Nearest Neighbors: Starting')\n",
    "start_ = datetime.datetime.now()\n",
    "neighbors = calculate_similarity_and_dump(paragraphs_embeddings, question_embeddings, s['slice_type'], q_to_p,\n",
    "                     os.path.join(datadir, 'dev_mean_glove_neighbors.csv'))\n",
    "end_ = datetime.datetime.now()\n",
    "print('Nearest Neighbors: Completed in {} minutes.'.format((end_-start_).seconds/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/10570 [00:00<03:29, 50.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [04:26<00:00, 39.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: Completed in 8.0 minutes.\n",
      "GLOVE + IDF Embeddings is completed in 42.4 minutes\n"
     ]
    }
   ],
   "source": [
    "mean_glove_with_idf_embeddings =  np.array([\n",
    "    np.mean([glove_word_weights[w] * token2idfweight[w]\n",
    "             for w in words if w in glove_word_weights] or\n",
    "            [np.zeros(dim)], axis=0)\n",
    "    for words in tokenized_questions+tokenized_paragraphs\n",
    "])\n",
    "####################################################################################################################\n",
    "##############       MEAN GLOVE WITH IDF EMBEDDINGS\n",
    "####################################################################################################################\n",
    "dump_embeddings(mean_glove_with_idf_embeddings, os.path.join(datadir,'dev_mean_glove_with_idf_embeddings.hdf5'))\n",
    "question_embeddings = mean_glove_with_idf_embeddings[0:len(tokenized_questions),:]\n",
    "paragraphs_embeddings = mean_glove_with_idf_embeddings[len(tokenized_questions):,:]\n",
    "print('Nearest Neighbors: Starting')\n",
    "start_ = datetime.datetime.now()\n",
    "neighbors = calculate_similarity_and_dump(paragraphs_embeddings, question_embeddings, s['slice_type'], q_to_p,\n",
    "                     os.path.join(datadir, 'dev_mean_glove_with_idf_neighbors.csv'))\n",
    "end_ = datetime.datetime.now()\n",
    "print('Nearest Neighbors: Completed in {} minutes.'.format((end_-start_).seconds/60))\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print('GLOVE + IDF Embeddings is completed in {} minutes'.format((end-start).seconds/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
