{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "path = !echo ${VIRTUAL_ENV}\n",
    "path = os.path.join(path[0], '..')\n",
    "sys.path.append(path)\n",
    "import helper.utils as UTIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET ARE LOADED WITH TOKENIZED FORM\n",
    "### THIS DOES NOT NEED TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating DEV examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n",
      "# of Paragraphs in DEV : 2067\n",
      "# of Questions in DEV : 10570\n",
      "# of Q_to_P DEV : 10570\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Paragraphs in DEV : 2067\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Questions in DEV : 10570\n",
      "Parsing Ended in 0.21666666666666667 minutes\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [01:11<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Dev Questions: 10570, Len of Dev Paragraphs: 2067\n",
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating TRAIN examples...\n",
      "87599 questions in total\n",
      "# of Paragraphs in TRAIN : 18896\n",
      "# of Questions in TRAIN : 87599\n",
      "# of Q_to_P TRAIN : 87599\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Paragraphs in TRAIN : 18896\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Questions in TRAIN : 87599\n",
      "Parsing Ended in 2.1333333333333333 minutes\n",
      "****************************************************************************************************\n",
      "Len of Train Questions: 87599, Len of Train Paragraphs: 18896\n"
     ]
    }
   ],
   "source": [
    "DIR = '/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/'\n",
    "args={\n",
    "'train_dataset_file': 'train-v1.1.json',\n",
    "'dev_dataset_file': 'dev-v1.1.json',\n",
    "}\n",
    "dev_tokenized_questions, \\\n",
    "dev_tokenized_paragraphs,\\\n",
    "dev_questions_nontokenized,\\\n",
    "dev_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['dev_dataset_file']), 'DEV')\n",
    "\n",
    "print('Len of Dev Questions: {}, Len of Dev Paragraphs: {}'.format(len(dev_tokenized_questions),\n",
    "                                                                  len(dev_tokenized_paragraphs)))\n",
    "\n",
    "train_tokenized_questions, \\\n",
    "train_tokenized_paragraphs,\\\n",
    "train_questions_nontokenized,\\\n",
    "train_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['train_dataset_file']), 'TRAIN')\n",
    "print('Len of Train Questions: {}, Len of Train Paragraphs: {}'.format(len(train_tokenized_questions),\n",
    "                                                                  len(train_tokenized_paragraphs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = dev_tokenized_questions + train_tokenized_questions\n",
    "tokenized_paragraphs = dev_tokenized_paragraphs + train_tokenized_paragraphs\n",
    "questions_nontokenized = dev_questions_nontokenized + train_questions_nontokenized\n",
    "paragraphs_nontokenized = dev_paragraphs_nontokenized + train_paragraphs_nontokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOU CAN EITHER USE ONE OF THE DEV/TRAIN OR COMBINATION  TOKENS ABOVE FOR THE PURPOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FOLLOWING PART, NEEDS TO BE RUN FOR EACH DATA FOLDER IN PARAM such as ULTIMATE_OLD_API_no_IDF_with_1_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0\n"
     ]
    }
   ],
   "source": [
    "PARAMS_PATH= \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/params.json\"\n",
    "params = UTIL.Params(PARAMS_PATH)\n",
    "_base_path = os.path.join(DIR, params.executor['data_dir'])\n",
    "print(_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute for question_training\n",
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0/question_training\n"
     ]
    }
   ],
   "source": [
    "KN_FILE_NAMES =UTIL.get_file_key_names_for_execution(params)\n",
    "print('Execute for {}'.format(KN_FILE_NAMES['DIR']))\n",
    "data_path = os.path.join(_base_path, KN_FILE_NAMES['DIR'])\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) TRAINING FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_embeddings=UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_TARGET_EMBEDDINGS\"]]))\n",
    "\n",
    "train_source_indx =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_IDX\"]])).astype(int)\n",
    "train_source_labels =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_LABELS\"]])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6725/6725 [00:08<00:00, 789.58it/s] \n"
     ]
    }
   ],
   "source": [
    "train_source_texts = []\n",
    "for indx in tqdm(train_source_indx):\n",
    "    train_question = questions_nontokenized[indx]\n",
    "    train_questions.append(train_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08684976, -0.04605952, -0.09545521, ...,  0.01791359,\n",
       "         0.0098526 ,  0.13276504],\n",
       "       [-0.09024484, -0.05345621,  0.04380247, ...,  0.09215055,\n",
       "         0.10844743,  0.16277342],\n",
       "       [-0.10331678,  0.06271477, -0.03893897, ...,  0.05493199,\n",
       "         0.01267613,  0.06338305],\n",
       "       ...,\n",
       "       [-0.10705659, -0.02247855, -0.12750474, ...,  0.06168772,\n",
       "        -0.08351889,  0.01065556],\n",
       "       [-0.04641121,  0.03306587, -0.1092279 , ..., -0.03879117,\n",
       "        -0.05317794, -0.09994104],\n",
       "       [-0.04497295, -0.02209207, -0.21226965, ..., -0.0476587 ,\n",
       "         0.13204622, -0.00755106]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(filename):\n",
    "    sr, audio = read(filename)\n",
    "    audio = audio.astype(float)\n",
    "    audio = audio - audio.min()\n",
    "    audio = audio / (audio.max() - audio.min())\n",
    "    audio = (audio - 0.5) * 2\n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, audio = get_audio(\"/home/jackalhan/Downloads/a2002011001-e02.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "frame_size = 2048\n",
    "frame_shift = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio =np.random.randn(10584000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio = training_audio[:sr*240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
