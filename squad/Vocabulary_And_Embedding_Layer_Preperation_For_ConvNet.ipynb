{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "path = !echo ${VIRTUAL_ENV}\n",
    "path = os.path.join(path[0], '..')\n",
    "sys.path.append(path)\n",
    "import helper.utils as UTIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET ARE LOADED WITH TOKENIZED FORM\n",
    "### THIS DOES NOT NEED TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating DEV examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n",
      "# of Paragraphs in DEV : 2067\n",
      "# of Questions in DEV : 10570\n",
      "# of Q_to_P DEV : 10570\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Paragraphs in DEV : 2067\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Questions in DEV : 10570\n",
      "Parsing Ended in 0.21666666666666667 minutes\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [01:11<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Dev Questions: 10570, Len of Dev Paragraphs: 2067\n",
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating TRAIN examples...\n",
      "87599 questions in total\n",
      "# of Paragraphs in TRAIN : 18896\n",
      "# of Questions in TRAIN : 87599\n",
      "# of Q_to_P TRAIN : 87599\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Paragraphs in TRAIN : 18896\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Questions in TRAIN : 87599\n",
      "Parsing Ended in 2.1333333333333333 minutes\n",
      "****************************************************************************************************\n",
      "Len of Train Questions: 87599, Len of Train Paragraphs: 18896\n"
     ]
    }
   ],
   "source": [
    "DIR = '/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/'\n",
    "args={\n",
    "'train_dataset_file': 'train-v1.1.json',\n",
    "'dev_dataset_file': 'dev-v1.1.json',\n",
    "}\n",
    "dev_tokenized_questions, \\\n",
    "dev_tokenized_paragraphs,\\\n",
    "dev_questions_nontokenized,\\\n",
    "dev_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['dev_dataset_file']), 'DEV')\n",
    "\n",
    "print('Len of Dev Questions: {}, Len of Dev Paragraphs: {}'.format(len(dev_tokenized_questions),\n",
    "                                                                  len(dev_tokenized_paragraphs)))\n",
    "\n",
    "train_tokenized_questions, \\\n",
    "train_tokenized_paragraphs,\\\n",
    "train_questions_nontokenized,\\\n",
    "train_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['train_dataset_file']), 'TRAIN')\n",
    "print('Len of Train Questions: {}, Len of Train Paragraphs: {}'.format(len(train_tokenized_questions),\n",
    "                                                                  len(train_tokenized_paragraphs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = dev_tokenized_questions + train_tokenized_questions\n",
    "tokenized_paragraphs = dev_tokenized_paragraphs + train_tokenized_paragraphs\n",
    "questions_nontokenized = dev_questions_nontokenized + train_questions_nontokenized\n",
    "paragraphs_nontokenized = dev_paragraphs_nontokenized + train_paragraphs_nontokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOU CAN EITHER USE ONE OF THE DEV/TRAIN OR COMBINATION  TOKENS ABOVE FOR THE PURPOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FOLLOWING PART, NEEDS TO BE RUN FOR EACH DATA FOLDER IN PARAM such as ULTIMATE_OLD_API_no_IDF_with_1_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0\n"
     ]
    }
   ],
   "source": [
    "PARAMS_PATH= \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/params.json\"\n",
    "params = UTIL.Params(PARAMS_PATH)\n",
    "_base_path = os.path.join(DIR, params.executor['data_dir'])\n",
    "print(_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute for question_training\n",
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0/question_training\n"
     ]
    }
   ],
   "source": [
    "KN_FILE_NAMES =UTIL.get_file_key_names_for_execution(params)\n",
    "print('Execute for {}'.format(KN_FILE_NAMES['DIR']))\n",
    "data_path = os.path.join(_base_path, KN_FILE_NAMES['DIR'])\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) TRAINING FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_embeddings=UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_TARGET_EMBEDDINGS\"]]))\n",
    "\n",
    "train_source_indx =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_IDX\"]])).astype(int)\n",
    "train_source_labels =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_LABELS\"]])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6725/6725 [00:08<00:00, 789.58it/s] \n"
     ]
    }
   ],
   "source": [
    "train_source_texts = []\n",
    "for indx in tqdm(train_source_indx):\n",
    "    train_question = questions_nontokenized[indx]\n",
    "    train_questions.append(train_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08684976, -0.04605952, -0.09545521, ...,  0.01791359,\n",
       "         0.0098526 ,  0.13276504],\n",
       "       [-0.09024484, -0.05345621,  0.04380247, ...,  0.09215055,\n",
       "         0.10844743,  0.16277342],\n",
       "       [-0.10331678,  0.06271477, -0.03893897, ...,  0.05493199,\n",
       "         0.01267613,  0.06338305],\n",
       "       ...,\n",
       "       [-0.10705659, -0.02247855, -0.12750474, ...,  0.06168772,\n",
       "        -0.08351889,  0.01065556],\n",
       "       [-0.04641121,  0.03306587, -0.1092279 , ..., -0.03879117,\n",
       "        -0.05317794, -0.09994104],\n",
       "       [-0.04497295, -0.02209207, -0.21226965, ..., -0.0476587 ,\n",
       "         0.13204622, -0.00755106]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(filename):\n",
    "    sr, audio = read(filename)\n",
    "    audio = audio.astype(float)\n",
    "    audio = audio - audio.min()\n",
    "    audio = audio / (audio.max() - audio.min())\n",
    "    audio = (audio - 0.5) * 2\n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, audio = get_audio(\"/home/jackalhan/Downloads/a2002011001-e02.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "frame_size = 2048\n",
    "frame_shift = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio =np.random.randn(10584000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio = training_audio[:sr*240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "path = !echo ${VIRTUAL_ENV}\n",
    "path = os.path.join(path[0], '..')\n",
    "sys.path.append(path)\n",
    "import helper.utils as UTIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator:\n",
    "    def __init__(self, file, table_name='embeddings'):\n",
    "        self.file = file\n",
    "        self.table_name = table_name\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            for im in hf[self.table_name]:\n",
    "                yield im\n",
    "\n",
    "def _read_dataset(file_path, embedding_dim, table_name='embeddings', data_type=tf.float32):\n",
    "    if embedding_dim is not None:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([embedding_dim, ]))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([]))\n",
    "    return ds\n",
    "\n",
    "def _parser_non_conv(source_embeddings,\n",
    "                     target_embeddings,\n",
    "                     target_labels):\n",
    "    features = {\"source_embeddings\": source_embeddings}\n",
    "    labels = {\"target_embeddings\": target_embeddings,\n",
    "              \"target_labels\": target_labels}\n",
    "    return features, labels\n",
    "\n",
    "def _repeater_fn(target_labels, repeat_list):\n",
    "    if target_labels not in repeat_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/ULTIMATE_OLD_API_with_IDF_with_1_0_0/\"\n",
    "_train_source_embeddings = os.path.join(path, \"question_training\",\"train_question_embeddings.hdf5\")\n",
    "_train_target_embeddings = os.path.join(path,\"question_training\", \"train_paragraph_embeddings.hdf5\")\n",
    "_train_source_labels = os.path.join(path,\"question_training\", \"train_question_labels.hdf5\")\n",
    "_train_source_embeddings = _read_dataset(_train_source_embeddings, 1024)\n",
    "_train_target_embeddings = _read_dataset(_train_target_embeddings, 1024)\n",
    "_train_source_labels = _read_dataset(_train_source_labels, None, data_type=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(class_size=20963, batch=512, buffer_size=84787):\n",
    "    dataset = tf.data.Dataset.zip((_train_source_embeddings, _train_target_embeddings, _train_source_labels))\n",
    "    target_list_np = np.random.choice(class_size, batch, replace=False)    \n",
    "    dataset = dataset.map(_parser_non_conv)      \n",
    "    #dataset = dataset.shuffle(buffer_size=buffer_size) \n",
    "    for i, t in enumerate(target_list_np):        \n",
    "        if i == 0:\n",
    "            dataset_ = dataset.filter(lambda features, labels: tf.equal(labels['target_labels'], t)).take(1)\n",
    "        else:\n",
    "            dataset_ = dataset_.concatenate(dataset.filter(lambda features, labels: tf.equal(labels['target_labels'], t)).take(1))\n",
    "    dataset = dataset_.shuffle(batch)\n",
    "    dataset = dataset_.batch(batch)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return next_element, target_list_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "_next, par = input_fn(batch=4)\n",
    "with tf.Session() as sess:\n",
    "    value = sess.run(_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source_embeddings': array([[-1.1460828 ,  1.4028684 , -0.8121466 , ..., -0.75059336,\n",
       "          -1.9347275 ,  1.570022  ],\n",
       "         [-2.88856   ,  0.7939497 , -0.5279569 , ..., -1.0717467 ,\n",
       "           0.7037743 ,  2.2476268 ],\n",
       "         [-1.5454668 , -0.9402997 , -1.7358644 , ..., -0.07986278,\n",
       "           2.4861035 , -2.1018429 ],\n",
       "         [-0.02267063,  2.6228843 , -2.280973  , ...,  1.4978906 ,\n",
       "           1.65165   ,  0.39760858]], dtype=float32)},\n",
       " {'target_embeddings': array([[ 0.21672936,  0.60549283, -0.5099839 , ...,  1.2391334 ,\n",
       "           0.22140406, -0.2875904 ],\n",
       "         [-0.7677321 , -0.18804908,  0.5015572 , ...,  0.33911023,\n",
       "           1.1989979 , -0.92873794],\n",
       "         [-0.07599598,  0.3036891 , -0.23900305, ...,  0.8791997 ,\n",
       "           0.37414524, -0.3102086 ],\n",
       "         [-1.295609  ,  0.17239891,  1.1195781 , ...,  1.0096842 ,\n",
       "           1.1128237 , -0.6416027 ]], dtype=float32),\n",
       "  'target_labels': array([ 1292, 18653,  6116,  9956])})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1292, 18653,  6116,  9956])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.randint(25, size=100000)\n",
    "data = np.random.randint(100000, size=100000)\n",
    "uniqs = tf.constant(np.random.randint(50, size=50))\n",
    "def input_fn():    \n",
    "    batch_size = 25\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, labels))#.repeat(lambda data, labels: tf.constant(1))\n",
    "    ds = ds.shuffle(100000)\n",
    "    for indx, l in enumerate(np.unique(labels)):\n",
    "        #print(\"{} -> {}\".format(indx, l))\n",
    "        if indx == 0:\n",
    "            ds_a = ds.filter(lambda data, label: tf.equal(label, l)).take(1)\n",
    "\n",
    "        else:\n",
    "            ds_a = ds_a.concatenate(ds.filter(lambda data, label: tf.equal(label, l)).take(1))\n",
    "    # def mapping(data, labels):\n",
    "    #     output, idx = tf.unique_with_counts(labels, out_idx=tf.int64)\n",
    "    # output_labels = ds.map(mapping)\n",
    "    #ds_a = ds_a.shuffle(2)\n",
    "    ds_a = ds_a.batch(batch_size)\n",
    "    ds = ds.prefetch(1)\n",
    "    ds_x = ds_a.make_one_shot_iterator()\n",
    "    next_element = ds_x.get_next()\n",
    "    return next_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-f45ba416f742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-fd48c0bec717>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mds_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mds_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmake_one_shot_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0m_make_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"Cannot capture a stateful node\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_make_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapture_by_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1639\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m     return gen_dataset_ops.batch_dataset(\n\u001b[0;32m-> 1641\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     return gen_dataset_ops.concatenate_dataset(\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_to_concatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     return gen_dataset_ops.concatenate_dataset(\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_to_concatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     return gen_dataset_ops.concatenate_dataset(\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_to_concatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         output_shapes=nest.flatten(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_as_variant_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_to_concatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         output_shapes=nest.flatten(\n\u001b[0;32m-> 1364\u001b[0;31m             sparse.as_dense_shapes(self.output_shapes, self.output_classes)),\n\u001b[0m\u001b[1;32m   1365\u001b[0m         output_types=nest.flatten(\n\u001b[1;32m   1366\u001b[0m             sparse.as_dense_types(self.output_types, self.output_classes)))\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[0m\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[1;32m   1380\u001b[0m     ])\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36moutput_shapes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         for (ts1, ts2) in zip(\n\u001b[1;32m   1378\u001b[0m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n\u001b[0m\u001b[1;32m   1380\u001b[0m     ])\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     return nest.pack_sequence_as(self._input_dataset.output_shapes, [\n\u001b[1;32m   1376\u001b[0m         \u001b[0mts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         for (ts1, ts2) in zip(\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             nest.flatten(self._dataset_to_concatenate.output_shapes))\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmost_specific_compatible_shape\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0munknown_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mndims\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mndims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;34m\"\"\"Returns the rank of this shape, or None if it is unspecified.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next_element = input_fn()\n",
    "sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
