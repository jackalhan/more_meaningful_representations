{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "path = !echo ${VIRTUAL_ENV}\n",
    "path = os.path.join(path[0], '..')\n",
    "sys.path.append(path)\n",
    "import helper.utils as UTIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET ARE LOADED WITH TOKENIZED FORM\n",
    "### THIS DOES NOT NEED TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating DEV examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n",
      "# of Paragraphs in DEV : 2067\n",
      "# of Questions in DEV : 10570\n",
      "# of Q_to_P DEV : 10570\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Paragraphs in DEV : 2067\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in DEV\n",
      "# of Tokenized Questions in DEV : 10570\n",
      "Parsing Ended in 0.21666666666666667 minutes\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [01:11<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Dev Questions: 10570, Len of Dev Paragraphs: 2067\n",
      "****************************************************************************************************\n",
      "Parsing Started\n",
      "Generating TRAIN examples...\n",
      "87599 questions in total\n",
      "# of Paragraphs in TRAIN : 18896\n",
      "# of Questions in TRAIN : 87599\n",
      "# of Q_to_P TRAIN : 87599\n",
      "--------------------\n",
      "Paragraphs: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Paragraphs in TRAIN : 18896\n",
      "--------------------\n",
      "Questions: Tokenization and Saving Tokenization Started in TRAIN\n",
      "# of Tokenized Questions in TRAIN : 87599\n",
      "Parsing Ended in 2.1333333333333333 minutes\n",
      "****************************************************************************************************\n",
      "Len of Train Questions: 87599, Len of Train Paragraphs: 18896\n"
     ]
    }
   ],
   "source": [
    "DIR = '/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/'\n",
    "args={\n",
    "'train_dataset_file': 'train-v1.1.json',\n",
    "'dev_dataset_file': 'dev-v1.1.json',\n",
    "}\n",
    "dev_tokenized_questions, \\\n",
    "dev_tokenized_paragraphs,\\\n",
    "dev_questions_nontokenized,\\\n",
    "dev_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['dev_dataset_file']), 'DEV')\n",
    "\n",
    "print('Len of Dev Questions: {}, Len of Dev Paragraphs: {}'.format(len(dev_tokenized_questions),\n",
    "                                                                  len(dev_tokenized_paragraphs)))\n",
    "\n",
    "train_tokenized_questions, \\\n",
    "train_tokenized_paragraphs,\\\n",
    "train_questions_nontokenized,\\\n",
    "train_paragraphs_nontokenized = UTIL.prepare_squad_objects(os.path.join(DIR, args['train_dataset_file']), 'TRAIN')\n",
    "print('Len of Train Questions: {}, Len of Train Paragraphs: {}'.format(len(train_tokenized_questions),\n",
    "                                                                  len(train_tokenized_paragraphs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = dev_tokenized_questions + train_tokenized_questions\n",
    "tokenized_paragraphs = dev_tokenized_paragraphs + train_tokenized_paragraphs\n",
    "questions_nontokenized = dev_questions_nontokenized + train_questions_nontokenized\n",
    "paragraphs_nontokenized = dev_paragraphs_nontokenized + train_paragraphs_nontokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOU CAN EITHER USE ONE OF THE DEV/TRAIN OR COMBINATION  TOKENS ABOVE FOR THE PURPOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE FOLLOWING PART, NEEDS TO BE RUN FOR EACH DATA FOLDER IN PARAM such as ULTIMATE_OLD_API_no_IDF_with_1_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0\n"
     ]
    }
   ],
   "source": [
    "PARAMS_PATH= \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/params.json\"\n",
    "params = UTIL.Params(PARAMS_PATH)\n",
    "_base_path = os.path.join(DIR, params.executor['data_dir'])\n",
    "print(_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute for question_training\n",
      "/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/DEV_OLD_API_no_IDF_with_1_0_0/question_training\n"
     ]
    }
   ],
   "source": [
    "KN_FILE_NAMES =UTIL.get_file_key_names_for_execution(params)\n",
    "print('Execute for {}'.format(KN_FILE_NAMES['DIR']))\n",
    "data_path = os.path.join(_base_path, KN_FILE_NAMES['DIR'])\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) TRAINING FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_embeddings=UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_TARGET_EMBEDDINGS\"]]))\n",
    "\n",
    "train_source_indx =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_IDX\"]])).astype(int)\n",
    "train_source_labels =UTIL.load_embeddings(os.path.join(data_path,\n",
    "                                                   params.files[\"train_loss\"][KN_FILE_NAMES[\"KN_SOURCE_LABELS\"]])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6725/6725 [00:08<00:00, 789.58it/s] \n"
     ]
    }
   ],
   "source": [
    "train_source_texts = []\n",
    "for indx in tqdm(train_source_indx):\n",
    "    train_question = questions_nontokenized[indx]\n",
    "    train_questions.append(train_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08684976, -0.04605952, -0.09545521, ...,  0.01791359,\n",
       "         0.0098526 ,  0.13276504],\n",
       "       [-0.09024484, -0.05345621,  0.04380247, ...,  0.09215055,\n",
       "         0.10844743,  0.16277342],\n",
       "       [-0.10331678,  0.06271477, -0.03893897, ...,  0.05493199,\n",
       "         0.01267613,  0.06338305],\n",
       "       ...,\n",
       "       [-0.10705659, -0.02247855, -0.12750474, ...,  0.06168772,\n",
       "        -0.08351889,  0.01065556],\n",
       "       [-0.04641121,  0.03306587, -0.1092279 , ..., -0.03879117,\n",
       "        -0.05317794, -0.09994104],\n",
       "       [-0.04497295, -0.02209207, -0.21226965, ..., -0.0476587 ,\n",
       "         0.13204622, -0.00755106]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(filename):\n",
    "    sr, audio = read(filename)\n",
    "    audio = audio.astype(float)\n",
    "    audio = audio - audio.min()\n",
    "    audio = audio / (audio.max() - audio.min())\n",
    "    audio = (audio - 0.5) * 2\n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, audio = get_audio(\"/home/jackalhan/Downloads/a2002011001-e02.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "frame_size = 2048\n",
    "frame_shift = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio =np.random.randn(10584000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_audio = training_audio[:sr*240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10584000, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "path = !echo ${VIRTUAL_ENV}\n",
    "path = os.path.join(path[0], '..')\n",
    "sys.path.append(path)\n",
    "import helper.utils as UTIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator:\n",
    "    def __init__(self, file, table_name='embeddings'):\n",
    "        self.file = file\n",
    "        self.table_name = table_name\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            for im in hf[self.table_name]:\n",
    "                yield im\n",
    "\n",
    "def _read_dataset(file_path, embedding_dim, table_name='embeddings', data_type=tf.float32):\n",
    "    if embedding_dim is not None:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([embedding_dim, ]))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([]))\n",
    "    return ds\n",
    "\n",
    "def _parser_non_conv(source_embeddings,\n",
    "                     target_embeddings,\n",
    "                     target_labels):\n",
    "    features = {\"source_embeddings\": source_embeddings}\n",
    "    labels = {\"target_embeddings\": target_embeddings,\n",
    "              \"target_labels\": target_labels}\n",
    "    return features, labels\n",
    "\n",
    "def _repeater_fn(target_labels, repeat_list):\n",
    "    if target_labels not in repeat_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/ULTIMATE_OLD_API_with_IDF_with_1_0_0/\"\n",
    "_train_source_embeddings = os.path.join(path, \"question_training\",\"train_question_embeddings.hdf5\")\n",
    "_train_target_embeddings = os.path.join(path,\"question_training\", \"train_paragraph_embeddings.hdf5\")\n",
    "_train_source_labels = os.path.join(path,\"question_training\", \"train_question_labels.hdf5\")\n",
    "_train_source_embeddings = _read_dataset(_train_source_embeddings, 1024)\n",
    "_train_target_embeddings = _read_dataset(_train_target_embeddings, 1024)\n",
    "_train_source_labels = _read_dataset(_train_source_labels, None, data_type=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(class_size=20963, batch=512, buffer_size=84787):\n",
    "    dataset = tf.data.Dataset.zip((_train_source_embeddings, _train_target_embeddings, _train_source_labels))\n",
    "    target_list_np = np.random.choice(class_size, batch, replace=False)    \n",
    "    dataset = dataset.map(_parser_non_conv)      \n",
    "    #dataset = dataset.shuffle(buffer_size=buffer_size) \n",
    "    for i, t in enumerate(target_list_np):        \n",
    "        if i == 0:\n",
    "            dataset_ = dataset.filter(lambda features, labels: tf.equal(labels['target_labels'], t)).take(1)\n",
    "        else:\n",
    "            dataset_ = dataset_.concatenate(dataset.filter(lambda features, labels: tf.equal(labels['target_labels'], t)).take(1))\n",
    "    dataset = dataset_.shuffle(batch)\n",
    "    dataset = dataset_.batch(batch)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return next_element, target_list_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "_next, par = input_fn(batch=4)\n",
    "with tf.Session() as sess:\n",
    "    value = sess.run(_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source_embeddings': array([[-1.1460828 ,  1.4028684 , -0.8121466 , ..., -0.75059336,\n",
       "          -1.9347275 ,  1.570022  ],\n",
       "         [-2.88856   ,  0.7939497 , -0.5279569 , ..., -1.0717467 ,\n",
       "           0.7037743 ,  2.2476268 ],\n",
       "         [-1.5454668 , -0.9402997 , -1.7358644 , ..., -0.07986278,\n",
       "           2.4861035 , -2.1018429 ],\n",
       "         [-0.02267063,  2.6228843 , -2.280973  , ...,  1.4978906 ,\n",
       "           1.65165   ,  0.39760858]], dtype=float32)},\n",
       " {'target_embeddings': array([[ 0.21672936,  0.60549283, -0.5099839 , ...,  1.2391334 ,\n",
       "           0.22140406, -0.2875904 ],\n",
       "         [-0.7677321 , -0.18804908,  0.5015572 , ...,  0.33911023,\n",
       "           1.1989979 , -0.92873794],\n",
       "         [-0.07599598,  0.3036891 , -0.23900305, ...,  0.8791997 ,\n",
       "           0.37414524, -0.3102086 ],\n",
       "         [-1.295609  ,  0.17239891,  1.1195781 , ...,  1.0096842 ,\n",
       "           1.1128237 , -0.6416027 ]], dtype=float32),\n",
       "  'target_labels': array([ 1292, 18653,  6116,  9956])})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1292, 18653,  6116,  9956])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.randint(15, size=100000)\n",
    "data = np.random.randint(100000, size=100000)\n",
    "uniqs = tf.constant(np.random.randint(50, size=50))\n",
    "def input_fn():    \n",
    "    batch_size = 15\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, labels))#.repeat(lambda data, labels: tf.constant(1))\n",
    "    ds = ds.shuffle(100000)\n",
    "    for indx, l in enumerate(np.unique(labels)):\n",
    "        #print(\"{} -> {}\".format(indx, l))\n",
    "        if indx == 0:\n",
    "            ds_a = ds.filter(lambda data, label: tf.equal(label, l)).take(1)\n",
    "\n",
    "        else:\n",
    "            ds_a = ds_a.concatenate(ds.filter(lambda data, label: tf.equal(label, l)).take(1))\n",
    "    # def mapping(data, labels):\n",
    "    #     output, idx = tf.unique_with_counts(labels, out_idx=tf.int64)\n",
    "    # output_labels = ds.map(mapping)\n",
    "    #ds_a = ds_a.shuffle(2)\n",
    "    ds_a = ds_a.batch(batch_size)\n",
    "    ds = ds.prefetch(1)\n",
    "    ds_x = ds_a.make_one_shot_iterator()\n",
    "    next_element = ds_x.get_next()\n",
    "    return next_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11627, 75583, 44010, 23477, 77503, 97555, 66559,  1537, 15016,\n",
       "        29649, 45565, 74861, 48780, 61157, 24843]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_element = input_fn()\n",
    "sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "    return mask\n",
    "\n",
    "def pairwise_euclidean_distances(A, B):\n",
    "    \"\"\"\n",
    "    Computes pairwise distances between each elements of A and each elements of B.\n",
    "    Args:\n",
    "      A,    [m,d] matrix\n",
    "      B,    [n,d] matrix\n",
    "    Returns:\n",
    "      D,    [m,n] matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('pairwise_euclidean_dist'):\n",
    "        # squared norms of each row in A and B\n",
    "        na = tf.reduce_sum(tf.square(A), 1)\n",
    "        nb = tf.reduce_sum(tf.square(B), 1)\n",
    "        # na as a row and nb as a co\"lumn vectors\n",
    "        na = tf.reshape(na, [-1, 1])\n",
    "        nb = tf.reshape(nb, [1, -1])\n",
    "        # return pairwise euclidead difference matrix\n",
    "        D = tf.sqrt(tf.maximum(na - 2 * tf.matmul(A, B, False, True) + nb, 0.0))\n",
    "    return D\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array([[2.0], [1.0], [1.5]])\n",
    "paragraphs = np.array([[1.7], [2.2], [1.6]])\n",
    "questions = tf.constant(questions)\n",
    "paragraphs = tf.constant(paragraphs)\n",
    "pairwise_dist = pairwise_euclidean_distances(questions, paragraphs)\n",
    "labels = tf.constant(np.array([0,1,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper.utils as UTIL\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class generator:\n",
    "    def __init__(self, file, table_name='embeddings'):\n",
    "        self.file = file\n",
    "        self.table_name = table_name\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            for im in hf[self.table_name]:\n",
    "                yield im\n",
    "\n",
    "def _read_dataset(file_path, embedding_dim, table_name='embeddings', data_type=tf.float32):\n",
    "    if embedding_dim is not None:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([embedding_dim, ]))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator(file_path, table_name),\n",
    "            data_type,\n",
    "            tf.TensorShape([]))\n",
    "    return ds\n",
    "\n",
    "def _parser_non_conv(source_embeddings,\n",
    "                     target_embeddings,\n",
    "                     target_labels):\n",
    "    features = {\"source_embeddings\": source_embeddings}\n",
    "    labels = {\"target_embeddings\": target_embeddings,\n",
    "              \"target_labels\": target_labels}\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/jackalhan/Development/github/more_meaningful_representations/squad/train/improvement/data/ULTIMATE_OLD_API_with_IDF_with_1_0_0/\"\n",
    "_train_source_embeddings = os.path.join(path, \"question_training\",\"train_question_embeddings.hdf5\")\n",
    "_train_target_embeddings = os.path.join(path,\"question_training\", \"train_paragraph_embeddings.hdf5\")\n",
    "_train_source_labels = os.path.join(path,\"question_training\", \"train_question_labels.hdf5\")\n",
    "_train_source_embeddings = _read_dataset(_train_source_embeddings, 1024)\n",
    "_train_target_embeddings = _read_dataset(_train_target_embeddings, 1024)\n",
    "_train_source_labels = _read_dataset(_train_source_labels, None, data_type=tf.int64)\n",
    "dataset = tf.data.Dataset.zip((_train_source_embeddings, _train_target_embeddings, _train_source_labels))\n",
    "dataset = dataset.map(_parser_non_conv)\n",
    "dataset = dataset.shuffle(buffer_size=84787)\n",
    "dataset = dataset.batch(512)\n",
    "dataset = dataset.prefetch(1)\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    value = sess.run(iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source_embeddings': array([[ 2.2237833 , -1.3224869 ,  0.75197136, ...,  0.4368565 ,\n",
       "           0.2647308 ,  1.6065657 ],\n",
       "         [ 0.11685371, -0.06194253, -1.7284765 , ..., -0.6862279 ,\n",
       "           5.1174684 , -1.9197305 ],\n",
       "         [ 1.7203722 , -1.2036293 , -1.4209306 , ...,  0.34072575,\n",
       "          -0.97558296,  1.7013804 ],\n",
       "         ...,\n",
       "         [ 1.8593748 , -1.4215288 ,  2.1808283 , ...,  1.4953336 ,\n",
       "           2.8870714 ,  1.5490513 ],\n",
       "         [-0.27243063, -4.152441  ,  3.6684186 , ...,  2.255337  ,\n",
       "           0.11426005,  1.0289307 ],\n",
       "         [ 0.25954056,  0.8691064 ,  0.9970252 , ..., -0.51615566,\n",
       "          -0.7907783 , -1.3220274 ]], dtype=float32)},\n",
       " {'target_embeddings': array([[-0.13039003, -0.48007065,  0.39660338, ...,  0.06883996,\n",
       "           0.74850756, -0.0830908 ],\n",
       "         [-0.56140375, -0.15490162, -0.6359981 , ..., -0.5233808 ,\n",
       "           1.1622634 ,  0.13134189],\n",
       "         [ 0.868584  , -0.64078873,  0.04704151, ...,  0.55717283,\n",
       "          -1.9916077 ,  1.6534297 ],\n",
       "         ...,\n",
       "         [-0.804715  ,  0.01337126,  1.6728455 , ...,  0.6403324 ,\n",
       "           0.9080488 , -1.6176373 ],\n",
       "         [ 0.30344003, -0.21026313,  0.70446604, ...,  1.0309973 ,\n",
       "           0.6870674 ,  0.60666066],\n",
       "         [ 0.32109833, -1.1592944 , -0.3551169 , ...,  0.99815094,\n",
       "          -1.0702388 ,  0.11268734]], dtype=float32),\n",
       "  'target_labels': array([16551,  6742,  8515, 19226, 19848, 15764,  3387,  5420,  5898,\n",
       "          4683, 17220, 14517,  5805, 15633, 11637,  4970,  5581,  6736,\n",
       "          8328, 20317, 10677,  9023,  9012,   890, 15471, 10149,  7341,\n",
       "          8243, 19548,  1196, 13331,  2304, 10883,  3237,  9702, 12928,\n",
       "          7556,  1191, 12839,  4463,  2015, 12322,  1473,  7763,  9429,\n",
       "         18253,  8480, 15970, 17675, 13567, 16357,  1965, 12696,  1509,\n",
       "          6843, 10550, 12139, 17102, 20841,  5199, 19410,  9131, 20059,\n",
       "         20743,    80,  2953,  6213,   616,  9061, 13327,  1437,  3088,\n",
       "          7095,  7760,  2698,  8704,  8990, 20612,  4289,  2199, 18570,\n",
       "         12848,  4630,  9998, 16296,  7115,  8522, 15657,  2714, 17867,\n",
       "         11185, 10126, 16602,  9735,  9846, 16136, 10218,  6560, 14040,\n",
       "          5429, 10798, 17870, 14085,   291,  4553,  6999,  7458, 13307,\n",
       "         16681, 12435, 15554,  5721,  1310, 17507, 19986, 14705, 16441,\n",
       "         11397,  7684, 15616, 14592, 12141, 19734,  1753, 12914, 14721,\n",
       "         17508,  4905,  9551,   583,  3197, 13864,  5838, 12960, 14506,\n",
       "         15443,   813,  8618, 12331,  1533, 16154, 14527,   496,  7879,\n",
       "         12974,  9469, 12365, 14879,  6711, 12739,  2899, 17884, 11054,\n",
       "         13914,  8635,  5154,  7093, 10131, 15660, 13394,  5747, 11154,\n",
       "         10631,  5779, 13868,  7127,  7907, 17097,  1482, 12984,  8970,\n",
       "         17403,  5591, 14344,  2232,  6043,  7029,   475, 18945,  3498,\n",
       "          9400,  3946, 18224, 14934, 19588,  2693, 15858,  4830, 15711,\n",
       "         19754,   929,    52, 13040,  5545, 19866,  3034, 19002, 16811,\n",
       "          8764, 13487, 16237,  8698, 17051, 14431, 14791, 19612,  5191,\n",
       "           772,  5825, 12285, 16930,  9774,   525,  1776, 18800,  4755,\n",
       "         12548, 16344,  7451,  9036,  2522, 16864,   991, 19442, 14071,\n",
       "          4278,  9332,  1775,  3270,  4753,  1516,  9005,  1938, 11026,\n",
       "         20717, 16450, 20822,  3616,  8782, 13979, 12419, 12858,  1224,\n",
       "         19310, 15539,  9155,  6809,  9083, 12537,  7813,   668, 19333,\n",
       "         17468, 15582,  5492, 13000,   801, 13341,  8986, 18593, 12174,\n",
       "         17870, 12339, 18694, 12469, 11748,  9714,  1547, 10622, 14545,\n",
       "           393,  5363, 11581,  4012,  8404,  4779,  2299,  8003, 20815,\n",
       "          1801,  9312, 19848,  8767, 13706,  9197, 18197,  9775,  6521,\n",
       "         18762, 17288, 12517,  2474,  4150,  1961,  2972, 13485, 18663,\n",
       "         13741, 10420, 16700, 12443,  8030,  3351,  2369,  4321,  2018,\n",
       "         10221,  6811, 12048, 10285, 13988, 15874, 13019, 10236, 15025,\n",
       "         10894,  2048,  1700,  3759, 19477,  9159, 16361, 12969, 17670,\n",
       "          4922, 14921, 18370,  1522,  4845,  7974,  8393,  9148,  9451,\n",
       "          1145, 14733, 10754,  9724, 18835,  7257, 19115, 12776, 15687,\n",
       "         11864, 10431,  9653, 10171,  9169, 16725, 20602, 14413,  5991,\n",
       "          8356,  2617,  1538, 18512, 19536,   154, 19169,  6297, 12401,\n",
       "          1845, 14765, 10323,  5709,  7540, 17134, 15691,  6186,  6634,\n",
       "         13117, 20293, 16969, 17294, 19072, 17029, 18757,  1346, 19405,\n",
       "         16285,  6413, 13152, 20144,  6570,  3171,   631,  9113,   805,\n",
       "          4596,  8621,  3017,  9871,  2604, 14606, 20415,  4978, 10865,\n",
       "          5200,  8182,  9897, 18219, 15574,  5696,  2114,  8959,  1032,\n",
       "          2329, 12871, 12345,  9151,  1736, 12161,  3405,  2753,  4429,\n",
       "         10701, 10846, 18034, 16031, 16275, 12598, 13991,  5478, 20567,\n",
       "          9467, 18564,  5538,  3908, 11516, 18335, 12991,  2515, 17147,\n",
       "         18180,  2002,  2392, 17928, 18365,  5070,  7129, 18466, 13554,\n",
       "          6063,  2739,   836,  7679, 10607,  2081,  8562,  7145,  4981,\n",
       "         16319, 18376,  6422,  9984,  2283, 13037,  6612, 20325, 15272,\n",
       "           455, 16472,  8091, 15645, 15777, 14621, 12282, 10913, 12929,\n",
       "          6753,  3332, 15792, 12544,  2134,  8260, 17735, 12579, 15879,\n",
       "          4830, 10397, 12564, 13203,  5627,  9062,  5407, 20192, 13397,\n",
       "         11930, 13472, 18907, 17386,  8205, 11635, 12602, 10047,  9516,\n",
       "         10830, 17549, 11223,  8029,  1838,  7280,  9795,  2499, 10076,\n",
       "         10986,  3076, 13807,  3104, 17598,  2590, 14551,  7661])})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "    return mask\n",
    "\n",
    "def pairwise_euclidean_distances(A, B):\n",
    "    \"\"\"\n",
    "    Computes pairwise distances between each elements of A and each elements of B.\n",
    "    Args:\n",
    "      A,    [m,d] matrix\n",
    "      B,    [n,d] matrix\n",
    "    Returns:\n",
    "      D,    [m,n] matrix of pairwise distances\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('pairwise_euclidean_dist'):\n",
    "        # squared norms of each row in A and B\n",
    "        na = tf.reduce_sum(tf.square(A), 1)\n",
    "        nb = tf.reduce_sum(tf.square(B), 1)\n",
    "        # na as a row and nb as a co\"lumn vectors\n",
    "        na = tf.reshape(na, [-1, 1])\n",
    "        nb = tf.reshape(nb, [1, -1])\n",
    "        # return pairwise euclidead difference matrix\n",
    "        D = tf.sqrt(tf.maximum(na - 2 * tf.matmul(A, B, False, True) + nb, 0.0))\n",
    "    return D\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array([[1.0], [1.0], [1.5], [4.0],[3.0],[4.0]])\n",
    "paragraphs = np.array([[1.2], [1.2], [1.4], [4.50],[3.5],[4.50]])\n",
    "questions = tf.constant(questions)\n",
    "paragraphs = tf.constant(paragraphs)\n",
    "pairwise_dist = pairwise_euclidean_distances(questions, paragraphs)\n",
    "labels = tf.constant(np.array([10,10,20,30,40,30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.4, 3.5, 2.5, 3.5],\n",
       "       [0.2, 0.2, 0.4, 3.5, 2.5, 3.5],\n",
       "       [0.3, 0.3, 0.1, 3. , 2. , 3. ],\n",
       "       [2.8, 2.8, 2.6, 0.5, 0.5, 0.5],\n",
       "       [1.8, 1.8, 1.6, 1.5, 0.5, 1.5],\n",
       "       [2.8, 2.8, 2.6, 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(pairwise_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "mask_anchor_positive = tf.to_float(mask_anchor_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.2, 0. , 0. , 0. , 0. ],\n",
       "       [0.2, 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.5, 0. , 0. ]], dtype=float32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "sess.run(anchor_positive_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0.5],\n",
       "       [0. ],\n",
       "       [0.5]], dtype=float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(hardest_positive_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "anchor_negative_dist = tf.multiply(mask_anchor_negative, pairwise_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0.4, 3.5, 2.5, 3.5],\n",
       "       [0. , 0. , 0.4, 3.5, 2.5, 3.5],\n",
       "       [0.3, 0.3, 0. , 3. , 2. , 3. ],\n",
       "       [2.8, 2.8, 2.6, 0. , 0.5, 0. ],\n",
       "       [1.8, 1.8, 1.6, 1.5, 0. , 1.5],\n",
       "       [2.8, 2.8, 2.6, 0. , 0.5, 0. ]], dtype=float32)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(anchor_negative_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0.5],\n",
       "       [0. ],\n",
       "       [0.5]], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(hardest_positive_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2],\n",
       "       [-0.2],\n",
       "       [-0.3],\n",
       "       [ 0. ],\n",
       "       [-1.5],\n",
       "       [ 0. ]], dtype=float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(true_targets - false_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.5],\n",
       "       [1.5],\n",
       "       [0.5]], dtype=float32)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(false_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2],\n",
       "       [0.2],\n",
       "       [0. ],\n",
       "       [0.5],\n",
       "       [0. ],\n",
       "       [0.5]], dtype=float32)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(true_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
