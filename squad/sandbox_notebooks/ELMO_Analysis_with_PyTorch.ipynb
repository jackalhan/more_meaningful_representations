{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackalhan/Development/github/more_meaningful_representations/venv36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/jackalhan/Development/github/more_meaningful_representations/venv36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE PATHS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_as_fake = True\n",
    "\n",
    "#dataset_type = 'train'\n",
    "dataset_type = 'dev'\n",
    "dataset_version = 'v1.1'\n",
    "\n",
    "\n",
    "_basepath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/'\n",
    "_options_file_name = 'elmo_2x4096_512_2048cnn_2xhighway_weights.json'\n",
    "_weight_file_name = 'elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "_vocab_file_name = '{}_voc.txt'.format(dataset_type)\n",
    "_embedding_paragraph_file_as_h5py_name = 'elmo_paragraph_embeddings.hdf5'\n",
    "_embedding_question_file_as_h5py_name = 'elmo_question_embeddings.hdf5'\n",
    "_paragraphs_file_name_as_txt = '{}_paragraphs.txt'\n",
    "_questions_file_name_as_txt = '{}_questions.txt'\n",
    "_nearest_all_cos_similarity_results_file_name =  '{}_slice_{}_nearest_all_cos_similarity.csv'\n",
    "\n",
    "datadir = os.path.join(_basepath, dataset_type)\n",
    "modeldir = os.path.join(_basepath, 'model')\n",
    "_squad_file_name = '{}-{}.json'.format(dataset_type, dataset_version)\n",
    "squad_file = os.path.join(datadir, _squad_file_name)\n",
    "vocab_file = os.path.join(datadir, _vocab_file_name)\n",
    "options_file = os.path.join(modeldir, _options_file_name)\n",
    "weight_file = os.path.join(modeldir, _weight_file_name)\n",
    "embedding_paragraph_file_as_h5py = os.path.join(datadir, _embedding_paragraph_file_as_h5py_name)\n",
    "embedding_question_file_as_h5py = os.path.join(datadir, _embedding_question_file_as_h5py_name)\n",
    "paragraphs_file_as_txt = os.path.join(datadir, _paragraphs_file_name_as_txt.format(dataset_type))\n",
    "questions_file_as_txt = os.path.join(datadir, _questions_file_name_as_txt.format(dataset_type))\n",
    "nearest_all_cos_similarity_results_file = os.path.join(datadir, _nearest_all_cos_similarity_results_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITIES :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "index_field = ['Unnamed: 0']\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "    \n",
    "\n",
    "\n",
    "def read_squad_data(squad_file_path):\n",
    "    #Read Dataset From Json File\n",
    "    with open(squad_file, 'r') as _squad:\n",
    "        squad = json.load(_squad)\n",
    "    # Parse, titles and contents from the data    \n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    _i_para, _i_qas = 0, 0\n",
    "    for _i_titles, _titles in enumerate(squad['data']):\n",
    "        for _paragraph in _titles['paragraphs']:\n",
    "            paragraphs.append(_paragraph['context'].replace('\\n', ''))\n",
    "            for _qas in _paragraph['qas']:\n",
    "                questions.append(_qas['question'].replace('\\n', ''))                                  \n",
    "                _i_qas += 1\n",
    "            _i_para+=1\n",
    "    return paragraphs, questions\n",
    "\n",
    "def read_fake_data(paragraphs_file_path, \n",
    "                   questions_file_path):\n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    with open(paragraphs_file_path, 'r') as fp_in, open(questions_file_path, 'r') as fq_in:\n",
    "        for i, line in enumerate(fp_in):\n",
    "            paragraphs.append(line.replace('\\n', ''))            \n",
    "        for i, line in enumerate(fq_in):\n",
    "            questions.append(line.replace('\\n', ''))            \n",
    "    return paragraphs, questions\n",
    "    \n",
    "\n",
    "def tokenize_contexts(contexts:list):\n",
    "    tokenized_context = [word_tokenize(_) for _ in contexts]\n",
    "    return tokenized_context\n",
    "\n",
    "def dump_tokenized_contexts(tokenized_contexts:list, file_path:str):\n",
    "    with open(file_path, 'w') as fout:\n",
    "        for context in tokenized_contexts:\n",
    "            fout.write(' '.join(context) + '\\n')\n",
    "            \n",
    "def create_voc(tokenized_contexts:list):\n",
    "    all_tokens = set(['<S>', '</S>', '<UNK>'])\n",
    "    for context in tokenized_contexts:\n",
    "        for token in context:\n",
    "            all_tokens.add(token)\n",
    "    return all_tokens\n",
    "\n",
    "def dump_voc(vocs:set, file_path:str):\n",
    "    with open(file_path, 'w') as fout:\n",
    "        fout.write('\\n'.join(vocs))\n",
    "        \n",
    "def create_and_dump_embeddings(embedder, \n",
    "                               tokenized_contexts_file_path:str, \n",
    "                               file_path_to_dump:str,\n",
    "                               embed_type='all'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    output_format : ``str``, optional, (default = \"all\")\n",
    "             The embeddings to output.  Must be one of \"all\", \"top\", or \"average\".\n",
    "    \"\"\"\n",
    "    with open(tokenized_contexts_file_path, 'r') as fin:         \n",
    "        ee.embed_file(fin, file_path_to_dump,output_format=embed_type)\n",
    "        \n",
    "def read_embeddings(embeddings_file_path, slice_index=None, axis=(0,1)):    \n",
    "    embeddings_=[]\n",
    "    keys = []\n",
    "    with h5py.File(embeddings_file_path, 'r') as fin:        \n",
    "        print('Embeddings are getting processed!')\n",
    "        for _ in tqdm_notebook(fin, total=len(fin)):\n",
    "            print(_.text.encode('ascii','ignore'))\n",
    "            keys.append(str(_))\n",
    "            vec = np.array(fin[str(_)][...])            \n",
    "            if slice_index is not None:\n",
    "                vec = vec[slice_index]\n",
    "            mean_vector = np.apply_over_axes(np.mean, vec, axis)\n",
    "            embeddings_.append(mean_vector) \n",
    "    embeddings = np.asarray(embeddings_)\n",
    "    return keys, embeddings \n",
    "\n",
    "def finding_nearest_neighbors(embedded_paragraphs_means, \n",
    "                              embedded_questions_means, \n",
    "                              questions, \n",
    "                              paragraphs,\n",
    "                              norm_type='l2'):\n",
    "    from sklearn.preprocessing import normalize\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print('Similarities are getting calculated !')   \n",
    "    nearest_neighbors = []\n",
    "    for q_id, _ in enumerate(tqdm_notebook(embedded_questions_means, total=len(embedded_questions_means))):\n",
    "        question = questions[q_id]\n",
    "        q_vec = np.array([_]) \n",
    "        if norm_type =='l2':\n",
    "            sk_sim = cosine_similarity(q_vec,embedded_paragraphs_means)[0]\n",
    "        else :\n",
    "            q_ = normalize(q_vec, norm='l1', axis=1)\n",
    "            p_ = normalize(embedded_paragraphs_means, norm='l1', axis=1)\n",
    "            sk_sim = np.dot(q_, p_.T)[0]\n",
    "        \n",
    "        similarities = np.argsort(-sk_sim)\n",
    "        order_of_the_actual_paragraph_id = np.where(similarities == q_id)[0][0] + 1\n",
    "        calculated_most_similar_1_paragraph = similarities[0]\n",
    "        for i, nearest_paragraph_id in enumerate(similarities[0:5]):\n",
    "            nearest_neighbors.append((question,\n",
    "                                       paragraphs[nearest_paragraph_id],\n",
    "                                       i+1, \n",
    "                                       sk_sim[nearest_paragraph_id] ))\n",
    "    return nearest_neighbors\n",
    "\n",
    "def dump_nearest_neighbors(nearest_neighbors:list, file_path:str):\n",
    "    df_nearest_neighbors = pd.DataFrame(data=nearest_neighbors, \n",
    "                                         columns=['question', \n",
    "                                                  'paragraph', \n",
    "                                                  'nearest_order', \n",
    "                                                  'cos_similarity'])\n",
    "    df_nearest_neighbors.to_csv(file_path, index=False)\n",
    "\n",
    "def traverse(o, tree_types=(list, tuple)):\n",
    "    if isinstance(o, tree_types):\n",
    "        for value in o:\n",
    "            for subvalue in traverse(value, tree_types):\n",
    "                yield subvalue\n",
    "    else:\n",
    "        yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs, questions = read_squad_data(squad_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND DUMP TOKENS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paragraphs = tokenize_contexts(paragraphs)\n",
    "dump_tokenized_contexts(tokenized_paragraphs, paragraphs_file_as_txt)\n",
    "\n",
    "tokenized_questions= tokenize_contexts(questions)\n",
    "dump_tokenized_contexts(tokenized_questions, questions_file_as_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND DUMP VOCABULARY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs = create_voc(tokenized_paragraphs + tokenized_questions)\n",
    "dump_voc(vocs, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKESET CREATION IF execute_as_fake == 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of tokens: 28155\n",
      "Taking 5000 tokens from the list\n"
     ]
    }
   ],
   "source": [
    "if execute_as_fake:\n",
    "    token_counts = Counter([token for token in traverse(tokenized_paragraphs + tokenized_questions)])\n",
    "    token_counts = [(k, token_counts[k]) for k in sorted(token_counts, key=token_counts.get, reverse=True)]\n",
    "    print('Total length of tokens: {}'.format(len(token_counts)))\n",
    "    sanity_tokens = token_counts[100:5100]\n",
    "    print('Taking {} tokens from the list'.format(len(sanity_tokens)))\n",
    "    sanity_tokens = [[k] for k, v in sanity_tokens]\n",
    "    dump_tokenized_contexts(sanity_tokens, paragraphs_file_as_txt)\n",
    "    dump_tokenized_contexts(sanity_tokens, questions_file_as_txt)\n",
    "    paragraphs, questions = read_fake_data(paragraphs_file_as_txt, questions_file_as_txt)\n",
    "    dump_voc([k for k in traverse(sanity_tokens)], vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DUMP EMBEDDINGS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE ELMO EMBEDDER\n",
    "ee = ElmoEmbedder(options_file, weight_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: Before execute the following line to create embeddings as a batch, \n",
    "you need to make changes in the **embed_file** function of **elmo.py** file of the codes.\n",
    "The reason of doing it is that, instead of create datasets with token names in H5PY file, I am using indexes to store datasets in the file, therefore here is the small modification: \n",
    "\n",
    "**-> Line:285**\n",
    "\n",
    "**Original Code:**\n",
    "\n",
    "```python\n",
    "for key, embeddings in Tqdm.tqdm(embedded_sentences):\n",
    "    ...\n",
    "    ...\n",
    "    fout.create_dataset(key,\n",
    "                        output.shape, dtype='float32',\n",
    "                        data=output)\n",
    "```\n",
    "\n",
    "**Updated Code:**\n",
    "\n",
    "```python\n",
    "for i, embeddings_ in enumerate(Tqdm.tqdm(embedded_sentences)):\n",
    "    key = embeddings_[0]\n",
    "    embeddings = embeddings_[1]\n",
    "    ...\n",
    "    ...\n",
    "    fout.create_dataset(str(i),\n",
    "                        output.shape, dtype='float32',\n",
    "                        data=output)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4999it [00:38, 130.93it/s]\n",
      "4999it [00:34, 145.82it/s]\n"
     ]
    }
   ],
   "source": [
    "create_and_dump_embeddings(ee, paragraphs_file_as_txt, embedding_paragraph_file_as_h5py)\n",
    "create_and_dump_embeddings(ee, questions_file_as_txt, embedding_question_file_as_h5py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  EMBEDDINGS:\n",
    "#### SLICE CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings will be executed by the following configs: \n",
      "{'slice_type': 'All', 'slice_index': None, 'axis': (0, 1)}\n"
     ]
    }
   ],
   "source": [
    "dims = 1024\n",
    "slices = [{'slice_type':'All',\n",
    "              'slice_index':None,\n",
    "              'axis':(0,1)},\n",
    "          {'slice_type':'1st',\n",
    "              'slice_index':0,\n",
    "              'axis':(0)},\n",
    "          {'slice_type':'2nd',\n",
    "              'slice_index':1,\n",
    "              'axis':(0)},\n",
    "          {'slice_type':'3rd',\n",
    "              'slice_index':2,\n",
    "              'axis':(0)}]\n",
    "\n",
    "selected_slice_conf =slices[0]\n",
    "\n",
    "print('Embeddings will be executed by the following configs: \\n{}'.format(selected_slice_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### READ EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are getting processed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4998), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c583c74e59c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m paragraph_keys, embedded_paragraphs = read_embeddings(embedding_paragraph_file_as_h5py, \n\u001b[1;32m      3\u001b[0m                                       \u001b[0mselected_slice_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slice_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                       selected_slice_conf['axis'])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Paragraphs shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_paragraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m embedded_paragraphs_means_with_all_slices = np.reshape(embedded_paragraphs, \n",
      "\u001b[0;32m<ipython-input-19-d1046300cdad>\u001b[0m in \u001b[0;36mread_embeddings\u001b[0;34m(embeddings_file_path, slice_index, axis)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embeddings are getting processed!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# -------------------------- Paragraphs\n",
    "paragraph_keys, embedded_paragraphs = read_embeddings(embedding_paragraph_file_as_h5py, \n",
    "                                      selected_slice_conf['slice_index'], \n",
    "                                      selected_slice_conf['axis'])\n",
    "print('Paragraphs shape', embedded_paragraphs.shape)\n",
    "embedded_paragraphs_means_with_all_slices = np.reshape(embedded_paragraphs, \n",
    "                                                      (embedded_paragraphs.shape[0], dims))\n",
    "print('Paragraphs shape', embedded_paragraphs_means_with_all_slices.shape)\n",
    "# -------------------------- Questions\n",
    "question_keys,embedded_questions = read_embeddings(embedding_question_file_as_h5py,\n",
    "                                     selected_slice_conf['slice_index'], \n",
    "                                     selected_slice_conf['axis'])\n",
    "print('Questions shape', embedded_questions.shape)\n",
    "embedded_questions_means_with_all_slices = np.reshape(embedded_questions, \n",
    "                                                      (embedded_questions.shape[0], dims))\n",
    "print('Questions shape', embedded_questions_means_with_all_slices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND and DUMP NEAREST NEIGHBORS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = finding_nearest_neighbors(embedded_paragraphs_means_with_all_slices, \n",
    "                          embedded_questions_means_with_all_slices, \n",
    "                          question_keys, \n",
    "                          paragraph_keys,\n",
    "                          norm_type='l2')\n",
    "\n",
    "dump_nearest_neighbors(nearest_neighbors, \n",
    "                       nearest_all_cos_similarity_results_file.format(dataset_type,selected_slice_conf['slice_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paragraph_keys' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-689f2dcc95be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparagraph_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'paragraph_keys' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
