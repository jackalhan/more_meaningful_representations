{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_type = 'dev'\n",
    "\n",
    "_basepath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/dev_voc_and_dev_q_p_ELMO_with_IDF_NOT_SMOOTH_NOT_SUBLINEARED'\n",
    "datadir = os.path.join(_basepath)\n",
    "\n",
    "_neighbors_file_name = '{}_neighbors.csv'.format(dataset_type)\n",
    "neighbors_file = os.path.join(datadir, _neighbors_file_name)\n",
    "\n",
    "\n",
    "def recall_at_k(r, k):\n",
    "    \"\"\"Score is recall @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> recall_at(r, 1)\n",
    "    0.0\n",
    "    >>> recall_at(r, 2)\n",
    "    0.0\n",
    "    >>> recall_at(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r_ = np.asarray(r)\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.sum(r[r[:]>0])/np.sum(r_[r_[:]>0])\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "def dcg2_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg2_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    idcg = dcg2_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg2_at_k(r, k) / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = pd.read_csv(neighbors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo_with_idf_weights_a_0.42_b_0.43_c0.15_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_1_b_0_c0_output_neighbors.csv\n",
      "elmo_only_weights_a_0.48_b_0.35_c0.17_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.09_b_0.07_c0.84_output_neighbors.csv\n",
      "elmo_only_weights_a_0.09_b_0.02_c0.89_output_neighbors.csv\n",
      "elmo_only_weights_a_0.31_b_0.22_c0.47_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0_b_1_c0_output_neighbors.csv\n",
      "elmo_only_weights_a_0.43_b_0.24_c0.33_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.65_b_0.08_c0.27_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.53_b_0.41_c0.06_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.07_b_0.51_c0.42_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.3_b_0.08_c0.62_output_neighbors.csv\n",
      "elmo_only_weights_a_0.32_b_0.08_c0.6_output_neighbors.csv\n",
      "elmo_only_weights_a_0.32_b_0.58_c0.1_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.68_b_0.15_c0.17_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.60_b_0.25_c0.15_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.49_b_0.32_c0.19_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.2_b_0.18_c0.62_output_neighbors.csv\n",
      "elmo_only_weights_a_0.34_b_0.37_c0.29_output_neighbors.csv\n",
      "elmo_only_weights_a_0.26_b_0.15_c0.59_output_neighbors.csv\n",
      "elmo_only_weights_a_0.11_b_0.0_c0.89_output_neighbors.csv\n",
      "elmo_only_weights_a_0_b_0_c1_output_neighbors.csv\n",
      "glove_only_output_neighbors.csv\n",
      "elmo_only_weights_a_1_b_0_c0_output_neighbors.csv\n",
      "elmo_only_weights_a_0.77_b_0.11_c0.12_output_neighbors.csv\n",
      "elmo_only_weights_a_0.02_b_0.86_c0.12_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.6_b_0.15_c0.24_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.03_b_0.31_c0.66_output_neighbors.csv\n",
      "elmo_only_weights_a_0.19_b_0.7_c0.11_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.21_b_0.23_c0.56_output_neighbors.csv\n",
      "glove_with_idf_output_neighbors.csv\n",
      "elmo_only_weights_a_0.51_b_0.22_c0.27_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0_b_0_c1_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.5_b_0.25_c0.25_output_neighbors.csv\n",
      "elmo_with_idf_weights_a_0.21_b_0.41_c0.38_output_neighbors.csv\n",
      "elmo_only_weights_a_0.11_b_0.4_c0.49_output_neighbors.csv\n",
      "elmo_only_weights_a_0.26_b_0.14_c0.60_output_neighbors.csv\n",
      "elmo_only_weights_a_0_b_1_c0_output_neighbors.csv\n"
     ]
    }
   ],
   "source": [
    "mypath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Weights'\n",
    "for (dirpath, dirnames, filenames) in os.walk(mypath):    \n",
    "    performance = []\n",
    "    for each_file in filenames:\n",
    "        print(each_file)\n",
    "        neighbors = pd.read_csv(os.path.join(dirpath, each_file))\n",
    "        neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "        neighbors.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "        #total_relevant_docs = neighbors[neighbors['Is_Actual_Paragraph'] == 1]['Is_Actual_Paragraph'].count()\n",
    "        \n",
    "        for k in [1,2,5,10,20,50]:\n",
    "            top_k = neighbors[neighbors['neighbor_order'] <= k]\n",
    "            len_of_received_relevant_doc = top_k[top_k['Is_Actual_Paragraph'] == 1]['Is_Actual_Paragraph'].count()\n",
    "    #         recall_k = len_of_received_relevant_doc/total_relevant_docs \n",
    "\n",
    "    #         top_k_grouped = top_k.groupby('question').head(k)\n",
    "    #         sub_recall_k, sub_precision_k, sub_dcg_k , sub_ndcg_k, sub_ndcg2_k,sub_avg_precision_i = 0,0,0,0,0,0\n",
    "    #         print(10*'-')\n",
    "    #         print('K: {}'.format(k))\n",
    "    #         for _ in range(total_relevant_docs):\n",
    "\n",
    "    #             each_group_set = top_k_grouped[_*k:_*k+k]\n",
    "    #             #sub_recall_k += recall_at_k(each_group_set['Is_Actual_Paragraph'].values, k)\n",
    "    #             sub_precision_k += precision_at_k(each_group_set['Is_Actual_Paragraph'].values, k)    \n",
    "    #             sub_dcg_k += dcg_at_k(each_group_set['Is_Actual_Paragraph'].values, k)\n",
    "    #             sub_ndcg_k += ndcg_at_k(each_group_set['Is_Actual_Paragraph'].values, k)\n",
    "    #             #sub_ndcg2_k += ndcg2_at_k(each_group_set['Is_Actual_Paragraph'].values, k)\n",
    "    #             sub_avg_precision_i = average_precision(each_group_set['Is_Actual_Paragraph'].values)\n",
    "\n",
    "            performance.append((k, each_file.replace('_output_neighbors.csv', ''), len_of_received_relevant_doc\n",
    "                                ))\n",
    "\n",
    "\n",
    "        #df_documents_recall_precision = pd.DataFrame(data=documents_recall_precision, columns=['top_n', 'recall', 'precision'])    \n",
    "    df_performance_model = pd.DataFrame(data=performance, columns=['top_n','conf', 'recall'])\n",
    "    df_performance_model.sort_values(by=['top_n', 'recall'], ascending=[True, False])\n",
    "    df_performance_model.to_csv(os.path.join(dirpath, 'performances.csv'))\n",
    "#         ax = df_performance_model.plot(kind='bar', title=each_file.replace('_output_neighbors.csv', ''));\n",
    "#         for p in ax.patches: \n",
    "#             ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()/2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "#         fig = ax.get_figure()\n",
    "#         fig.savefig(os.path.join(dirpath, 'performance_'+each_file + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-b48451ddb3ef>, line 9)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b48451ddb3ef>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    for i\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mypath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Weighs'\n",
    "for (dirpath, dirnames, filenames) in os.walk(mypath):\n",
    "    for each_file in filenames:\n",
    "        print(each_file)\n",
    "        neighbors = pd.read_csv(os.path.join(dirpath,each_file))\n",
    "        neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "        neighbors.sort_values(by=['neighbor_cos_similarity'], ascending=[False])\n",
    "        top_k = neighbors[neighbors['Is_Actual_Paragraph'] == 1]\n",
    "        for i\n",
    "        top_k_grouped = top_k.groupby(['Is_Actual_Paragraph']).count()\n",
    "        total_zeros =top_k_grouped[top_k_grouped.index == 0]['slice_type'].values[0]\n",
    "        total_1s =top_k_grouped[top_k_grouped.index == 1]['slice_type'].values[0]\n",
    "#         for k in [1,2,5,10,20,50]:\n",
    "#             top_k = neighbors[neighbors['Is_Actual_Paragraph'] <= 1]\n",
    "#             top_k_grouped = top_k.groupby(['Is_Actual_Paragraph']).count()\n",
    "#             total_zeros =top_k_grouped[top_k_grouped.index == 0]['slice_type'].values[0]\n",
    "#             total_1s =top_k_grouped[top_k_grouped.index == 1]['slice_type'].values[0]                        \n",
    "        performances = []\n",
    "        min_cutoff = min(top_k['neighbor_cos_similarity'])\n",
    "        for _cut_off in [float(x/10) for x in range(int(min_cutoff)*10,11)]:\n",
    "            grouped_greater = top_k[top_k['neighbor_cos_similarity'] >= _cut_off].groupby(['Is_Actual_Paragraph']).count()\n",
    "            try:\n",
    "                true_positive = grouped_greater[grouped_greater.index == 1]['slice_type'].values[0]\n",
    "            except:\n",
    "                true_positive = 0\n",
    "\n",
    "            grouped_smaller = top_k[top_k['neighbor_cos_similarity'] < _cut_off].groupby(['Is_Actual_Paragraph']).count()\n",
    "            try:\n",
    "                true_negative = grouped_smaller[grouped_smaller.index == 0]['slice_type'].values[0]\n",
    "            except:\n",
    "                true_negative = 0 \n",
    "\n",
    "            try:\n",
    "                false_positive = grouped_greater[grouped_greater.index == 0]['slice_type'].values[0]\n",
    "            except:\n",
    "                false_positive = 0\n",
    "\n",
    "            try:\n",
    "                false_negative = grouped_smaller[grouped_smaller.index == 1]['slice_type'].values[0]\n",
    "            except:\n",
    "                false_negative = 0\n",
    "\n",
    "            true_negative_rate = (true_negative/total_zeros)\n",
    "            false_positive_rate = 1 - true_negative_rate\n",
    "            true_positive_rate = (true_positive/total_1s)\n",
    "            try:\n",
    "                precision = true_positive / (true_positive + false_positive)\n",
    "            except:\n",
    "                precision = 0\n",
    "\n",
    "            try:\n",
    "                recall = true_positive / (true_positive + false_negative)\n",
    "            except:\n",
    "                recall = 0\n",
    "            k_performance = (_cut_off, true_positive, true_negative, false_positive, false_negative, precision, recall, true_negative_rate, false_positive_rate, true_positive_rate)\n",
    "            performances.append(k_performance)\n",
    "            \n",
    "        df_prediction_model = pd.DataFrame(data=performances, columns=['cut_off', 'True Positive', 'True Negative', 'False_Positive', 'False_Negative','Precision', 'Recall', 'True Negative Rate', 'False Positive Rate', 'True Positive Rate'])\n",
    "        ap = 0\n",
    "        previous_recall = 0\n",
    "        for i, each_row in df_prediction_model.iterrows():\n",
    "            print('Current Recall: {}'.format(each_row['Recall']))\n",
    "            print('Previous Recall: {}'.format(previous_recall))\n",
    "            current_ap = (each_row['Recall'] - previous_recall) * each_row['Precision']\n",
    "            print('Current AP: {}'.format(current_ap))\n",
    "\n",
    "            ap += current_ap\n",
    "            print('Total AP: {}'.format(ap))    \n",
    "            print(10*'-')\n",
    "            previous_recall = each_row['Recall']\n",
    "        print(ap)\n",
    "            #df_prediction_model['ap'] = ap\n",
    "        df_prediction_model.to_csv(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file))\n",
    "        #average_precision = average_precision_score(df_prediction_model['Recall'], df_prediction_model['Precision'])\n",
    "# \n",
    "        plt.step(df_prediction_model['Recall'], df_prediction_model['Precision'], color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "        plt.fill_between(df_prediction_model['Recall'], df_prediction_model['Precision'], step='post', alpha=0.2,\n",
    "                         color='b')\n",
    "#             plt.plot(df_prediction_model['Recall'], df_prediction_model['Recall'])\n",
    "#             plt.plot(df_prediction_model['Precision'], df_prediction_model['Precision'])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "#             plt.legend(['Recall', 'precision'],loc='upper left')\n",
    "        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "                  ap))\n",
    "        plt.savefig(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file+'.png'))\n",
    "# ---------------------------------------------------\n",
    "# ---------------------------------------------------\n",
    "# print('Top {} Items: {}'.format(i, top_k.shape[0]))\n",
    "# average_precision = average_precision_score(top_k['Is_Actual_Paragraph'], top_k['neighbor_cos_similarity'])\n",
    "# print('Average precision-recall score: {0:0.2f}'.format(\n",
    "#       average_precision))\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(top_k['Is_Actual_Paragraph'], top_k['neighbor_cos_similarity'])\n",
    "\n",
    "# plt.step(recall, precision, color='b', alpha=0.2,\n",
    "#          where='post')\n",
    "# plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "#                  color='b')\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "#           average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo_only_weights_a_1_b_1_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_only_weights_a_0.8_b_0.19999999999999996_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_with_idf_weights_a_0.7_b_0.30000000000000004_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_with_idf_weights_a_0.4_b_0.6_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_only_weights_a_0.9_b_0.09999999999999998_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_with_idf_weights_a_1_b_1_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_with_idf_weights_a_0.2_b_0.8_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_with_idf_weights_a_0.6_b_0.4_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_with_idf_weights_a_0.3_b_0.7_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_only_weights_a_0.1_b_0.9_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_with_idf_weights_a_0.1_b_0.9_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_with_idf_weights_a_0.9_b_0.09999999999999998_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "glove_only_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_with_idf_weights_a_0.8_b_0.19999999999999996_output_neighbors.csv\n",
      "Average precision-recall score: 0.03\n",
      "elmo_only_weights_a_0.3_b_0.7_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_only_weights_a_0.2_b_0.8_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_only_weights_a_0.6_b_0.4_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "elmo_only_weights_a_0.4_b_0.6_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n",
      "glove_with_idf_output_neighbors.csv\n",
      "Average precision-recall score: 0.04\n",
      "elmo_only_weights_a_0.7_b_0.30000000000000004_output_neighbors.csv\n",
      "Average precision-recall score: 0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff5550ca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mypath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Weighs'\n",
    "for (dirpath, dirnames, filenames) in os.walk(mypath):\n",
    "    for each_file in filenames:\n",
    "        print(each_file)\n",
    "        neighbors = pd.read_csv(os.path.join(dirpath,each_file))\n",
    "        neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "        neighbors.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "        top_50 = neighbors.copy() #neighbors[neighbors['neighbor_order'] <= 50]\n",
    "        \n",
    "        #print('Top {} Items: {}'.format(i, top_k.shape[0]))\n",
    "        average_precision = average_precision_score(top_50['Is_Actual_Paragraph'], top_50['neighbor_cos_similarity'])\n",
    "        print('Average precision-recall score: {0:0.2f}'.format(\n",
    "              average_precision))\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(top_50['Is_Actual_Paragraph'], top_50['neighbor_cos_similarity'])\n",
    "\n",
    "        plt.step(recall, precision, color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                         color='b')   \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.legend([each_file.replace('_output_neighbors.csv', '')],loc=\"lower right\")\n",
    "        plt.title('2-class Precision-Recall for curve: AP={0:0.2f}'.format(average_precision))\n",
    "#         number_of_relavent_docs = len(top_50[top_50['Is_Actual_Paragraph'] == 1])\n",
    "#         precision_recalls =[]\n",
    "#         retrieved_counter =0\n",
    "#         print('number_of_relavent_docs:{}'.format(number_of_relavent_docs))\n",
    "#         print('top_50:{}'.format(len(top_50)))\n",
    "#         top_50.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "#         for i, rows in enumerate(top_50.iterrows()):\n",
    "#             index = rows[0]\n",
    "#             row = rows[1]\n",
    "#             if row['Is_Actual_Paragraph'] == 1:\n",
    "#                 retrieved_counter +=1\n",
    "#             precision_recalls.append((i+1, \n",
    "#                                       retrieved_counter/(i+1), \n",
    "#                                       retrieved_counter/number_of_relavent_docs, \n",
    "#                                       row['Is_Actual_Paragraph']))\n",
    "#         df_precision_recalls = pd.DataFrame(data=precision_recalls, columns=['k', \n",
    "#                                                                                  'Precision',\n",
    "#                                                                                  'Recall', \n",
    "#                                                                                  'Is_True_Pair'])\n",
    "#         prec_records = df_precision_recalls[df_precision_recalls['Is_True_Pair'] == 1]\n",
    "#         ap = prec_records['Precision'].sum() / len(prec_records)\n",
    "#         df_precision_recalls.to_csv(os.path.join(dirpath, 'performance_' +each_file))\n",
    "# #         plt.step(df_precision_recalls['Recall'], df_precision_recalls['Precision'], color='b', alpha=0.2,\n",
    "# #                  where='post')\n",
    "# #         plt.fill_between(df_precision_recalls['Recall'], df_precision_recalls['Precision'], step='post', alpha=0.2,\n",
    "# #                          color='b')\n",
    "#         #plt.plot(df_precision_recalls['Recall'], df_precision_recalls['Recall'], 'bo',\n",
    "#                  #df_precision_recalls['Precision'],df_precision_recalls['Precision'], 'k')\n",
    "#         #plt.plot(, df_precision_recalls['Precision'])\n",
    "  \n",
    "#         plt.plot(df_precision_recalls['Recall'], df_precision_recalls['Precision'], label='area = %0.2f' % ap, color=\"green\")\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.title('Precision Recall Curve for {}'.format(each_file.replace('_output_neighbors.csv', '')))\n",
    "#         plt.legend(loc=\"lower right\")\n",
    "#         #plt.show()\n",
    "# #         plt.xlabel('Recall')\n",
    "# #         plt.ylabel('Precision')\n",
    "# #         plt.ylim([0.0, 1.05])\n",
    "# #         plt.xlim([0.0, 1.0])\n",
    "# #         plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "# #                   ap))\n",
    "# #         plt.legend(['Recall', 'Precision'],loc='upper left')\n",
    "        plt.savefig(os.path.join(dirpath, 'performance_'+each_file+'.png'))\n",
    "        plt.clf()\n",
    "#         min_ = neighbors[neighbors['Is_Actual_Paragraph'] == 1]\n",
    "#         precision_recalls =[]\n",
    "#         min_cutoff = min(min_['neighbor_cos_similarity'])\n",
    "#         top_k = neighbors[neighbors['neighbor_cos_similarity'] >= min_cutoff]\n",
    "#         number_of_relavent_docs = len(top_k[top_k['Is_Actual_Paragraph'] == 1])\n",
    "#         retrieved_counter = 0\n",
    "#         for i, row in top_k.iterrows():\n",
    "#             if row['Is_Actual_Paragraph'] == 1:\n",
    "#                     retrieved_counter +=1\n",
    "#             precision_recalls.append((i+1, \n",
    "#                                       retrieved_counter/len(top_k), \n",
    "#                                       retrieved_counter/number_of_relavent_docs, \n",
    "#                                       row['Is_Actual_Paragraph']))\n",
    "\n",
    "#         df_precision_recalls = pd.DataFrame(data=precision_recalls, columns=['k', 'Precision', 'Recall', 'Is_True_Pair'])\n",
    "#         ap = df_precision_recalls['Precision'].sum() / sum(df_precision_recalls[df_precision_recalls['Is_True_Pair'] == 1])\n",
    "        \n",
    "# #         for _ in range(1,max_k+1):\n",
    "# #             top_k_ = top_k[top_k['neighbor_order'] <= _]\n",
    "# #             retrieved_counter = top_k_[top_k_['Is_Actual_Paragraph'] == 1]['Is_Actual_Paragraph'].count()\n",
    "# #             precision_recalls.append((k, \n",
    "# #                                       retrieved_counter/(max_k+1), \n",
    "# #                                       retrieved_counter/q, \n",
    "# #                                       retrieved_counter))\n",
    "\n",
    "# #         for i, row in top_k.iterrows():\n",
    "# #             if row['Is_Actual_Paragraph'] == 1:\n",
    "# #                 retrieved_counter +=1\n",
    "# #             precision_recalls.append((i, retrieved_counter/(i+1), retrieved_counter/len(top_k), row['Is_Actual_Paragraph']))\n",
    "                    \n",
    "        \n",
    "#         df_prediction_model.to_csv(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file))\n",
    "#         plt.step(df_precision_recalls['Recall'], df_precision_recalls['Precision'], color='b', alpha=0.2,\n",
    "#                  where='post')\n",
    "#         plt.fill_between(df_precision_recalls['Recall'], df_precision_recalls['Precision'], step='post', alpha=0.2,\n",
    "#                          color='b')\n",
    "# #             plt.plot(df_prediction_model['Recall'], df_prediction_model['Recall'])\n",
    "# #             plt.plot(df_prediction_model['Precision'], df_prediction_model['Precision'])\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "# #             plt.legend(['Recall', 'precision'],loc='upper left')\n",
    "#         plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "#                   ap))\n",
    "#         plt.savefig(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file+'.png'))\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/dev/comparisions/Weighs'\n",
    "for (dirpath, dirnames, filenames) in os.walk(mypath):\n",
    "    for each_file in filenames:\n",
    "        print(each_file)\n",
    "        neighbors = pd.read_csv(os.path.join(dirpath,each_file))\n",
    "        neighbors['Is_Actual_Paragraph'] = (neighbors['neighbor_paragraph'] == neighbors['actual_paragraph']).astype('int')\n",
    "        neighbors.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "        top_50 = neighbors.copy() #neighbors[neighbors['neighbor_order'] <= 50]\n",
    "        q = 10570\n",
    "        p = 2067\n",
    "        \n",
    "        top50_grouped = top_50.groupby('question')[\"neighbor_cos_similarity\"].nlargest(p)\n",
    "        for name, grouped in top50_grouped:\n",
    "            i, = np.where( grouped['Is_Actual_Paragraph']==1 )\n",
    "            precision = 1/(i+1)\n",
    "            recall = 1/\n",
    "            (i+1, \n",
    "#                                       retrieved_counter/(i+1), \n",
    "#                                       retrieved_counter/number_of_relavent_docs, \n",
    "#                                       row['Is_Actual_Paragraph']))\n",
    "        \n",
    "        #print('Top {} Items: {}'.format(i, top_k.shape[0]))\n",
    "        average_precision = average_precision_score(top_50['Is_Actual_Paragraph'], top_50['neighbor_cos_similarity'])\n",
    "        print('Average precision-recall score: {0:0.2f}'.format(\n",
    "              average_precision))\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(top_50['Is_Actual_Paragraph'], top_50['neighbor_cos_similarity'])\n",
    "\n",
    "        plt.step(recall, precision, color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                         color='b')   \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.legend([each_file.replace('_output_neighbors.csv', '')],loc=\"lower right\")\n",
    "        plt.title('2-class Precision-Recall for curve: AP={0:0.2f}'.format(average_precision))\n",
    "#         number_of_relavent_docs = len(top_50[top_50['Is_Actual_Paragraph'] == 1])\n",
    "#         precision_recalls =[]\n",
    "#         retrieved_counter =0\n",
    "#         print('number_of_relavent_docs:{}'.format(number_of_relavent_docs))\n",
    "#         print('top_50:{}'.format(len(top_50)))\n",
    "#         top_50.sort_values(by=['neighbor_cos_similarity'], ascending=[False], inplace=True)\n",
    "#         for i, rows in enumerate(top_50.iterrows()):\n",
    "#             index = rows[0]\n",
    "#             row = rows[1]\n",
    "#             if row['Is_Actual_Paragraph'] == 1:\n",
    "#                 retrieved_counter +=1\n",
    "#             precision_recalls.append((i+1, \n",
    "#                                       retrieved_counter/(i+1), \n",
    "#                                       retrieved_counter/number_of_relavent_docs, \n",
    "#                                       row['Is_Actual_Paragraph']))\n",
    "#         df_precision_recalls = pd.DataFrame(data=precision_recalls, columns=['k', \n",
    "#                                                                                  'Precision',\n",
    "#                                                                                  'Recall', \n",
    "#                                                                                  'Is_True_Pair'])\n",
    "#         prec_records = df_precision_recalls[df_precision_recalls['Is_True_Pair'] == 1]\n",
    "#         ap = prec_records['Precision'].sum() / len(prec_records)\n",
    "#         df_precision_recalls.to_csv(os.path.join(dirpath, 'performance_' +each_file))\n",
    "# #         plt.step(df_precision_recalls['Recall'], df_precision_recalls['Precision'], color='b', alpha=0.2,\n",
    "# #                  where='post')\n",
    "# #         plt.fill_between(df_precision_recalls['Recall'], df_precision_recalls['Precision'], step='post', alpha=0.2,\n",
    "# #                          color='b')\n",
    "#         #plt.plot(df_precision_recalls['Recall'], df_precision_recalls['Recall'], 'bo',\n",
    "#                  #df_precision_recalls['Precision'],df_precision_recalls['Precision'], 'k')\n",
    "#         #plt.plot(, df_precision_recalls['Precision'])\n",
    "  \n",
    "#         plt.plot(df_precision_recalls['Recall'], df_precision_recalls['Precision'], label='area = %0.2f' % ap, color=\"green\")\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.title('Precision Recall Curve for {}'.format(each_file.replace('_output_neighbors.csv', '')))\n",
    "#         plt.legend(loc=\"lower right\")\n",
    "#         #plt.show()\n",
    "# #         plt.xlabel('Recall')\n",
    "# #         plt.ylabel('Precision')\n",
    "# #         plt.ylim([0.0, 1.05])\n",
    "# #         plt.xlim([0.0, 1.0])\n",
    "# #         plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "# #                   ap))\n",
    "# #         plt.legend(['Recall', 'Precision'],loc='upper left')\n",
    "        plt.savefig(os.path.join(dirpath, 'performance_'+each_file+'.png'))\n",
    "        plt.clf()\n",
    "#         min_ = neighbors[neighbors['Is_Actual_Paragraph'] == 1]\n",
    "#         precision_recalls =[]\n",
    "#         min_cutoff = min(min_['neighbor_cos_similarity'])\n",
    "#         top_k = neighbors[neighbors['neighbor_cos_similarity'] >= min_cutoff]\n",
    "#         number_of_relavent_docs = len(top_k[top_k['Is_Actual_Paragraph'] == 1])\n",
    "#         retrieved_counter = 0\n",
    "#         for i, row in top_k.iterrows():\n",
    "#             if row['Is_Actual_Paragraph'] == 1:\n",
    "#                     retrieved_counter +=1\n",
    "#             precision_recalls.append((i+1, \n",
    "#                                       retrieved_counter/len(top_k), \n",
    "#                                       retrieved_counter/number_of_relavent_docs, \n",
    "#                                       row['Is_Actual_Paragraph']))\n",
    "\n",
    "#         df_precision_recalls = pd.DataFrame(data=precision_recalls, columns=['k', 'Precision', 'Recall', 'Is_True_Pair'])\n",
    "#         ap = df_precision_recalls['Precision'].sum() / sum(df_precision_recalls[df_precision_recalls['Is_True_Pair'] == 1])\n",
    "        \n",
    "# #         for _ in range(1,max_k+1):\n",
    "# #             top_k_ = top_k[top_k['neighbor_order'] <= _]\n",
    "# #             retrieved_counter = top_k_[top_k_['Is_Actual_Paragraph'] == 1]['Is_Actual_Paragraph'].count()\n",
    "# #             precision_recalls.append((k, \n",
    "# #                                       retrieved_counter/(max_k+1), \n",
    "# #                                       retrieved_counter/q, \n",
    "# #                                       retrieved_counter))\n",
    "\n",
    "# #         for i, row in top_k.iterrows():\n",
    "# #             if row['Is_Actual_Paragraph'] == 1:\n",
    "# #                 retrieved_counter +=1\n",
    "# #             precision_recalls.append((i, retrieved_counter/(i+1), retrieved_counter/len(top_k), row['Is_Actual_Paragraph']))\n",
    "                    \n",
    "        \n",
    "#         df_prediction_model.to_csv(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file))\n",
    "#         plt.step(df_precision_recalls['Recall'], df_precision_recalls['Precision'], color='b', alpha=0.2,\n",
    "#                  where='post')\n",
    "#         plt.fill_between(df_precision_recalls['Recall'], df_precision_recalls['Precision'], step='post', alpha=0.2,\n",
    "#                          color='b')\n",
    "# #             plt.plot(df_prediction_model['Recall'], df_prediction_model['Recall'])\n",
    "# #             plt.plot(df_prediction_model['Precision'], df_prediction_model['Precision'])\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "# #             plt.legend(['Recall', 'precision'],loc='upper left')\n",
    "#         plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "#                   ap))\n",
    "#         plt.savefig(os.path.join(dirpath, 'performance_k_'+ str(k) + '_'+each_file+'.png'))\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# roc_auc = auc(df_prediction_model['False Positive Rate'], df_prediction_model['True Positive Rate'])\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(df_prediction_model['False Positive Rate'].values, df_prediction_model['True Positive Rate'].values, color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.rcParams[\"figure.figsize\"] = [15,15]\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
