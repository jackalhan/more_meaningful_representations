{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# required files\n",
    "_basepath = '/home/jackalhan/Development/github/more_meaningful_representations/squad/'\n",
    "datadir = os.path.join(_basepath, dataset_type)\n",
    "\n",
    "_paragraphs_file_name_as_txt = '{}_paragraphs.txt'\n",
    "paragraphs_file_as_txt = os.path.join(datadir, _paragraphs_file_name_as_txt.format(dataset_type))\n",
    "sanity_paragraphs_file_as_txt = os.path.join(datadir, _paragraphs_file_name_as_txt.format('sanity_' + dataset_type ))\n",
    "\n",
    "_questions_file_name_as_txt = '{}_questions.txt'\n",
    "questions_file_as_txt = os.path.join(datadir, _questions_file_name_as_txt.format(dataset_type))\n",
    "sanity_questions_file_as_txt = os.path.join(datadir, _questions_file_name_as_txt.format('sanity_' + dataset_type ))\n",
    "\n",
    "_voc_file_name_as_txt = '{}_voc.txt'\n",
    "voc_file_name_as_txt = os.path.join(datadir, _voc_file_name_as_txt.format('sanity_' + dataset_type ))\n",
    "\n",
    "# _voc_counter_file_name_as_txt = '{}_voc_counter.txt'\n",
    "# voc_counter_file_name = os.path.join(datadir, _voc_counter_file_name_as_txt.format('sanity_' + dataset_type ))\n",
    "\n",
    "_qas_file_name = '{}_qas.csv'.format('sanity_' + dataset_type )\n",
    "qas_file = os.path.join(datadir, _qas_file_name)\n",
    "\n",
    "_glove_embed_file_name = 'glove.6B.300d.txt'\n",
    "glove_embed_file = os.path.join(datadir, _glove_embed_file_name)\n",
    "\n",
    "_embedding_paragraph_file_as_h5py_name = 'elmo_paragraph_embeddings.hdf5'\n",
    "embedding_paragraph_file_as_h5py = os.path.join(datadir, _embedding_paragraph_file_as_h5py_name)\n",
    "\n",
    "_embedding_question_file_as_h5py_name = 'elmo_question_embeddings.hdf5'\n",
    "embedding_question_file_as_h5py = os.path.join(datadir, _embedding_question_file_as_h5py_name)\n",
    "\n",
    "_embedding_mean_paragraph_file_as_h5py_name = 'elmo_mean_paragraph_embeddings.hdf5'\n",
    "embedding_mean_paragraph_file_as_h5py = os.path.join(datadir, _embedding_mean_paragraph_file_as_h5py_name)\n",
    "\n",
    "_embedding_mean_question_file_as_h5py_name = 'elmo_mean_question_embeddings.hdf5'\n",
    "embedding_mean_question_file_as_h5py = os.path.join(datadir, _embedding_mean_question_file_as_h5py_name)\n",
    "\n",
    "_cos_similarity_results_file_name =  '{}_cos_similarity_with_{}_norm_for_q_vs_para.csv'\n",
    "cos_similarity_results_file_name = os.path.join(datadir, _cos_similarity_results_file_name)\n",
    "\n",
    "_nearest_all_cos_similarity_results_file_name =  '{}_nearest_all_cos_similarity_with_{}_norm_for_q_vs_para.csv'\n",
    "nearest_all_cos_similarity_results_file = os.path.join(datadir, _nearest_all_cos_similarity_results_file_name)\n",
    "\n",
    "_cos_similarity_results_as_hist_file_name =  'histogram_{}_cos_similarity_with_{}_norm_for_q_vs_para.png'\n",
    "cos_similarity_results_as_hist_file = os.path.join(datadir, _cos_similarity_results_as_hist_file_name)\n",
    "\n",
    "df_qas = pd.read_csv(qas_file).set_index(index_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining paragraphs and questions\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Obtaining paragraphs and questions')\n",
    "paragraphs = []\n",
    "questions = []\n",
    "p_look_up = []\n",
    "q_look_up = []\n",
    "with open(sanity_paragraphs_file_as_txt, 'r') as fp_in, open(sanity_questions_file_as_txt, 'r') as fq_in:\n",
    "    for i, line in enumerate(fp_in):\n",
    "        paragraphs.append(line.replace('\\n', ' '))\n",
    "        p_look_up.append((i, line.replace('\\n','')))\n",
    "    for i, line in enumerate(fq_in):\n",
    "        questions.append(line.replace('\\n', ' '))\n",
    "        q_look_up.append((i, line.replace('\\n','')))\n",
    "print('Done')\n",
    "df_p_look_up = pd.DataFrame(data=p_look_up, columns=['id', 'paragraph']).set_index('id')\n",
    "df_q_look_up = pd.DataFrame(data=q_look_up, columns=['id', 'question']).set_index('id')\n",
    "tokenized_paragraphs = [word_tokenize(_) for _ in paragraphs]\n",
    "tokenized_questions = [word_tokenize(_) for _ in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_glove_weights(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            #print(splitLine)\n",
    "            word = splitLine[0]\n",
    "            #print(word)\n",
    "            \n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            print(word)\n",
    "            print(splitLine[1:])\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "     \n",
    "     \n",
    "weights = load_glove_weights(glove_embed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are getting processed!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraphs are getting processed!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities are getting calculated !!!\n",
      "QUES Shape (10000, 300)\n",
      "PARA Shape (10000, 300)\n",
      "********** L1 NORM **********\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** L2 NORM **********\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_embeddings(embedding_type):\n",
    "    if embedding_type == 'glove':\n",
    "        dims = 300\n",
    "    else:\n",
    "        dims = 1024\n",
    "        \n",
    "    items = [dict({'type':'Questions', \n",
    "                         'matrix': np.empty((0, dims), dtype=float),\n",
    "                        'source':tokenized_questions,\n",
    "                         'destination_file': embedding_mean_question_file_as_h5py}), \n",
    "                  dict({'type':'Paragraphs', \n",
    "                         'matrix': np.empty((0, dims), dtype=float),\n",
    "                        'source': tokenized_paragraphs,\n",
    "                         'destination_file': embedding_mean_paragraph_file_as_h5py})\n",
    "                  ]\n",
    "    for vals in items:\n",
    "        print(vals['type'], 'are getting processed!!!')    \n",
    "        with h5py.File(vals['destination_file'], 'w') as fout:        \n",
    "            for _ in tqdm_notebook(vals['source'], total=len(vals['source'])):\n",
    "                if embedding_type == 'glove':\n",
    "                    try:\n",
    "                        vec = np.array(weights[_[0]])\n",
    "                    except:\n",
    "                        vec = np.array(weights['unk'])\n",
    "                else:\n",
    "                    \n",
    "                #print(vec.shape)            \n",
    "                reshaped_vector = np.reshape(vec, (1,dims))\n",
    "                #print(reshaped_vector.shape)\n",
    "                vals['matrix'] = np.append(vals['matrix'], reshaped_vector, axis=0)\n",
    "            for i, _ in enumerate(tqdm_notebook(vals['matrix'], total=len(vals['matrix']))):\n",
    "                    ds = fout.create_dataset(\n",
    "                                    '{}'.format(i),\n",
    "                                    _.shape, dtype='float32',\n",
    "                                    data=_)  \n",
    "\n",
    "    print('Similarities are getting calculated !!!')\n",
    "    QUES = items[0]['matrix']\n",
    "    print('QUES Shape', QUES.shape)\n",
    "    PARA = items[1]['matrix']\n",
    "    print('PARA Shape', PARA.shape)\n",
    "    for norm_type in ['l1', 'l2']:\n",
    "        print(10*'*', norm_type.upper(),'NORM', 10*'*')\n",
    "        results = []\n",
    "        nearest_paragraphs = []\n",
    "        for q_id, _ in enumerate(tqdm_notebook(QUES, total=len(QUES))):\n",
    "            question = df_q_look_up[df_q_look_up.index == q_id].values[0][0]\n",
    "            q_vec = np.array([_]) \n",
    "            if norm_type =='l2':\n",
    "                sk_sim = cosine_similarity(q_vec,PARA)[0]\n",
    "            else:\n",
    "                q_ = normalize(q_vec, norm='l1', axis=1)\n",
    "                p_ = normalize(PARA, norm='l1', axis=1)\n",
    "                sk_sim = np.dot(q_, p_.T)[0]\n",
    "\n",
    "            actual_paragraph_id = df_qas[df_qas['Question_Id'] == q_id]['Paragraph_Id'].values[0]\n",
    "            similarities = np.argsort(-sk_sim)\n",
    "            order_of_the_actual_paragraph_id = np.where(similarities == actual_paragraph_id)[0][0] + 1\n",
    "            calculated_most_similar_1_paragraph = similarities[0]\n",
    "            results.append((q_id, actual_paragraph_id,  \n",
    "                            order_of_the_actual_paragraph_id, \n",
    "                            sk_sim[actual_paragraph_id], \n",
    "                            calculated_most_similar_1_paragraph, \n",
    "                            sk_sim[calculated_most_similar_1_paragraph]))\n",
    "            for i, nearest_paragraph_id in enumerate(similarities[0:5]):\n",
    "                nearest_paragraphs.append((question, \n",
    "                                           df_p_look_up[df_p_look_up.index == nearest_paragraph_id].values[0][0],\n",
    "                                           i+1, \n",
    "                                           sk_sim[nearest_paragraph_id] ))\n",
    "\n",
    "        df_nearest_paragraphs = pd.DataFrame(data=nearest_paragraphs, columns=['question', 'paragraph', 'nearest_order', 'cos_similarity'])\n",
    "        df_nearest_paragraphs.to_csv(nearest_all_cos_similarity_results_file.format(dataset_type, norm_type), index=False)\n",
    "\n",
    "        df_results= pd.DataFrame(data=results, columns=['Question_Id', 'Actual_Paragraph_Id', \n",
    "                                             'Order Index of Actual_Paragraph_Id in Similarities List',\n",
    "                                             'Similarity Score for Actual_Paragraph_Id',\n",
    "                                             'Calculated Top 1 Most Similar Paragraph', \n",
    "                                             'Similarity Score for Most Similar Paragraph'\n",
    "                                            ])\n",
    "        df_results.to_csv(cos_similarity_results_file_name.format(dataset_type, norm_type), index=False)\n",
    "        ax = df_results['Order Index of Actual_Paragraph_Id in Similarities List'].hist()\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(cos_similarity_results_as_hist_file.format(dataset_type, norm_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
