{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = tf.constant([[100, 110], [10, 20], [1000, 1100]])\n",
    "a = tf.random_uniform(shape=[3, 150,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.reduce_mean(a)\n",
    "c = tf.reduce_mean(a, axis=0)\n",
    "d = tf.reduce_mean(a, axis=1)\n",
    "e = tf.reduce_mean(a, axis=2)\n",
    "f = tf.reduce_mean(a, axis=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INPUT\")\n",
    "print(session.run(a))\n",
    "print(\"REDUCE MEAN\")\n",
    "print(session.run(b))\n",
    "print(\"REDUCE MEAN (shape)\")\n",
    "print(b.shape)\n",
    "print(\"REDUCE MEAN AXIS 0\")\n",
    "print(session.run(c))\n",
    "print(\"REDUCE MEAN AXIS 0 (shape)\")\n",
    "print(c.shape)\n",
    "print(\"REDUCE MEAN AXIS 1\")\n",
    "print(session.run(d))\n",
    "print(\"REDUCE MEAN AXIS 1 (shape)\")\n",
    "print(d.shape)\n",
    "print(\"REDUCE MEAN AXIS 2\")\n",
    "print(session.run(e))\n",
    "print(\"REDUCE MEAN AXIS 2 (shape)\")\n",
    "print(e.shape)\n",
    "print(\"REDUCE MEAN AXIS 0,1\")\n",
    "print(session.run(f))\n",
    "print(\"REDUCE MEAN AXIS 0,1 (shape)\")\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_type = 'train'\n",
    "dataset_type = 'dev'\n",
    "dataset_version = 'v1.1'\n",
    "\n",
    "index_field = ['Unnamed: 0']\n",
    "\n",
    "# required files\n",
    "_basepath = '/home/jackalhan/Development/github/bilm-tf/squad'\n",
    "datadir = os.path.join(_basepath, dataset_type)\n",
    "modeldir = os.path.join(_basepath, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'file.hdf5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# List all groups\n",
    "print(\"Keys: %s\" % f.keys())\n",
    "a_group_key = list(f.keys())[0]\n",
    "\n",
    "# Get the data\n",
    "data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_with_mean = []\n",
    "questions_with_mean = []\n",
    "items = [dict({'type':'question', \n",
    "                     'list':questions_with_mean, \n",
    "                     'source_file':embedding_question_file_as_h5py,\n",
    "                     'destination_file': embedding_mean_question_file_as_h5py}), \n",
    "              dict({'type':'paragraph', \n",
    "                     'list':paragraphs_with_mean, \n",
    "                     'source_file':embedding_paragraph_file_as_h5py,\n",
    "                     'destination_file': embedding_mean_paragraph_file_as_h5py})\n",
    "              ]\n",
    "for vals in items:\n",
    "    print(vals['type'], 'is getting processed!!!')\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    \n",
    "    with h5py.File(vals['source_file'], 'r') as fin:\n",
    "        data_as_array = np.array(fin)\n",
    "    \n",
    "    print(data_as_array.shape)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "         h5py.File(vals['destination_file'], 'w') as fout:\n",
    "            #for _ in tqdm_notebook(fin, total=len(fin)):\n",
    "                mean_vectors = sess.run(tf.reduce_mean(fin[str(_)][...], axis=[0,1]))\n",
    "                \n",
    "                data_as_array\n",
    "                x = tf.placeholder(tf.float32, [100, 250])\n",
    "                norm = tf.sqrt(tf.reduce_sum(tf.square(x), 1, keep_dims=True))\n",
    "                res = x / norm\n",
    "                vals['list'].append(mean_vector)\n",
    "                ds = fout.create_dataset(\n",
    "                        '{}'.format(_),\n",
    "                        mean_vector.shape, dtype='float32',\n",
    "                        data=mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q = np.empty((0, 8), dtype=float)\n",
    "q = np.append(q, [[2, 0, 1, 1, 0, 2, 1, 1]], axis=0)\n",
    "\n",
    "p = np.empty((0, 8), dtype=float)\n",
    "p = np.append(p, [[7, 5, -23, 0, 1, 1, 1, 1]], axis=0)\n",
    "p = np.append(p, [[2, 1, 1, 0, 1, 1, 1, 1]], axis=0)\n",
    "p = np.append(p, [[2, 1, 1, 0, 1, 1, 1, 0]], axis=0)\n",
    "p = np.append(p, [[2, 0, 1, 1, 0, 2, 1, 1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.spatial.distance as distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for q_id, _ in enumerate(tqdm_notebook(q, total=len(p))):\n",
    "    q_vec = np.array([_]) \n",
    "    sk_sim = cosine_similarity(q_vec,p)[0]\n",
    "    actual_paragraph_id = 0 #df_qas[df_qas['Question_Id'] == q_id]['Paragraph_Id'].values[0]\n",
    "    similarities = np.argsort(-sk_sim)\n",
    "    print('actual_paragraph_id', actual_paragraph_id)\n",
    "    print('sk_sim:', sk_sim)\n",
    "    print('similarities (argsort):', similarities)\n",
    "    order_of_the_actual_paragraph_id = np.where(similarities == actual_paragraph_id)[0][0] + 1\n",
    "    print('order_of_the_actual_paragraph_id:', order_of_the_actual_paragraph_id)\n",
    "    print('score for actual paragraph id:', sk_sim[actual_paragraph_id])\n",
    "    calculated_most_similar_1_paragraph = similarities[0]\n",
    "    print('calculated_most_similar_1_paragraph:', calculated_most_similar_1_paragraph)\n",
    "    print('score for most similar paragraph:', sk_sim[calculated_most_similar_1_paragraph])\n",
    "    #     results.append((q_id, actual_paragraph_id,  order_of_the_actual_paragraph_id, calculated_most_similar_1_paragraph, sk_sim[calculated_most_similar_1_paragraph]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8), (4, 8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4 into shape (3,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1bc7508d2b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4 into shape (3,1)"
     ]
    }
   ],
   "source": [
    "p_ = normalize(p, norm='l1', axis=1)\n",
    "p1_ = p/np.abs(p).sum(axis=1).reshape(3,1)\n",
    "np.abs(p).sum(axis=1).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ = normalize(q, norm='l1', axis=1)\n",
    "p_ = normalize(p, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17948718,  0.12820513, -0.58974359,  0.        ,  0.02564103,\n",
       "         0.02564103,  0.02564103,  0.02564103],\n",
       "       [ 0.25      ,  0.125     ,  0.125     ,  0.        ,  0.125     ,\n",
       "         0.125     ,  0.125     ,  0.125     ],\n",
       "       [ 0.28571429,  0.14285714,  0.14285714,  0.        ,  0.14285714,\n",
       "         0.14285714,  0.14285714,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01602564,  0.140625  ,  0.14285714,  0.1875    ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(q_, p_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05858481,  0.82158384,  0.76980036,  1.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(q,p)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
